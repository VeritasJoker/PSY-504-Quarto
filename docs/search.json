[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-post/index.html",
    "href": "posts/new-post/index.html",
    "title": "Millie",
    "section": "",
    "text": "Hello, I am Millie üêà\n\nprint(\"I code in R\")\n\n[1] \"I code in R\"\n\n\n\nprint(\"And in Python\")\n\nAnd in Python"
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html",
    "title": "Lab: Logistic Regression",
    "section": "",
    "text": "Assignment requirements:\n\nIf you are using Github (recommended), make sure to commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages, and share the link to your assignment with me. If not, you can also send me the rmd & rendered file via Canvas.\nIn this assignment, you will not need to code from scratch. Rather, you‚Äôll need to fill in code where needed. This assignment has a logisitic regression implementation for a scenario from EDA down to model comparison (and would be useful for whenever you may encounter such a situation in the future).\nI want the assignments to begin reflecting a bit more of how you‚Äôd be doing things on your own, where you have some prior knowledge and you figure other things out (by referring to documentation, etc.) . In addition to the rmd, I also want you to submit to me notes of anything new that you learn while finishing the assignment. And any pain-points, and we‚Äôll discuss more.\n\nNote:\n\nIf you are fitting a model, display the model output in a neatly formatted table. (The gt tidy and kable functions can help!). Modelsummary also looks good(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)\nMake sure that your plots are clearly labeled ‚Äì for all axes, titles, etc."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#data-general-social-survey",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#data-general-social-survey",
    "title": "Lab: Logistic Regression",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent‚Äôs answer to the following prompt:\n‚ÄúWe are faced with many problems in this country, none of which can be solved easily or inexpensively. I‚Äôm going to name some of these problems, and for each one I‚Äôd like you to tell me whether you think we‚Äôre spending too much money on it, too little money, or about the right amount‚Ä¶are we spending too much, too little, or about the right amount on mass transportation?‚Äù\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent‚Äôs answer to the following prompt:\n‚ÄúWe hear a lot of talk these days about liberals and conservatives. I‚Äôm going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?‚Äù\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#eda",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#eda",
    "title": "Lab: Logistic Regression",
    "section": "EDA",
    "text": "EDA\n\nLet‚Äôs begin by making a binary variable for respondents‚Äô views on spending on mass transportation. Create a new variable that is equal to ‚Äú1‚Äù if a respondent said spending on mass transportation is about right and ‚Äú0‚Äù otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(easystats)\nlibrary(broom)\nlibrary(emmeans)\nlibrary(marginaleffects)\nlibrary(performance)\nlibrary(arm)\nlibrary(modelsummary)\n\n\n\n\nCode\n# load data\ndata &lt;- read.csv(\"gss2016.csv\")\n\n\nFill in the ‚Äú____‚Äù below to encode the binary variable\n\n\nCode\ndata &lt;- data %&gt;%\n   mutate(mass_trans_spend_right = ifelse(natmass==\"About right\", 1, 0))\n\n\n\n\nCode\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if neederd\nmass_spend_long &lt;- mass_spend_summary %&gt;% mutate(opinion=\"Opinion\")\n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\nggplot(mass_spend_long, aes(x = opinion, y = proportion, fill = mass_trans_spend_right)) +\n geom_bar(stat='identity') +\n  geom_text(aes(label=proportion),\n            vjust=ifelse(mass_spend_long$mass_trans_spend_right==0, -7, 7)\n            ) + \n  labs(x=\"Views on mass transportation spending\", y=\"Proportion\")\n\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                           levels = c(\"Extremely liberal\",\"Liberal\", \"Slightly liberal\", \"Moderate\", \"Slghtly conservative\", \"Conservative\", \"Extrmly conservative\"),\n                           ordered = TRUE))\n\n\n\nMake a plot of the distribution of polviews\n\n\n\nCode\n#Get proportions, format, and produce a plot like you did previously for \n\nmass_spend_summary &lt;- data %&gt;%\n  count(polviews) %&gt;%\n  mutate(proportion = n / sum(n))\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if neederd\nmass_spend_long &lt;- mass_spend_summary %&gt;% mutate(opinion=\"Opinion\")\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\nggplot(mass_spend_long, aes(x = opinion, y = proportion, fill = polviews)) +\n geom_bar(stat='identity') + \n  labs(x=\"Views on mass transportation spending\", y=\"Proportion\")\n\n\n\n\n\n\n\n\n\n\nWhich political view occurs most frequently in this data set?\nModerate occurs most frequently.\n\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\n\n\n\nCode\nmass_spend_summary &lt;- data %&gt;%\n  count(polviews, mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\nmass_spend_summary$mass_trans_spend_right &lt;- as.factor(mass_spend_summary$mass_trans_spend_right)\n\nggplot(mass_spend_summary, aes(x = polviews, y = proportion, fill = mass_trans_spend_right)) +\n geom_bar(stat='identity', position='fill') +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + \n  labs(x=\"Political Views\", y=\"Satisfaction Proportion\")\n\n\n\n\n\n\n\n\n\nThe more conservative one‚Äôs political views are the more they think the amount of spending on mass transportation is correct.\n\nWe‚Äôd like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as ‚Äú89 or older‚Äù.\n\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values ‚Äú89 or older‚Äù with a single value.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(age = ifelse(age == \"89 or older\", 89, age), \n         age = as.numeric(age))\n\n\n\nPlot the frequency distribution of age.\n\n\n\nCode\nggplot(data, aes(x = age)) +\n  geom_histogram(bins=30, fill=\"lightblue\", color=\"grey\") +\n  labs(x=\"Age\", y=\"Frequency\")"
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#logistic-regression",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#logistic-regression",
    "title": "Lab: Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLet‚Äôs start by fitting a logistic regression model with just the intercept\n\n\n\nCode\nintercept_only_model &lt;- glm(\n  mass_trans_spend_right ~ 1,\n  family=binomial,\n  data=data\n  ) \n\nintercept_only_model %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.1190594\n0.0393685\n3.024229\n0.0024927\n\n\n\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two‚Äìwhat is the basic thing this probability tells us about?\n\n\n\nCode\nb0 &lt;- coef(intercept_only_model) # get coef\n\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) # logistic transform\n\nci_lower = b0 - 1.96 * 0.0393685\nci_upper = b0 + 1.96 * 0.0393685\n\n#transforming confidence intervals of coefficients into probabilities\np_lower = exp(ci_lower) / (1 + exp(ci_lower))\np_upper = exp(ci_upper) / (1 + exp(ci_upper))\n\n\nlogit_to_prob &lt;- function(logit){\n  return(exp(logit) / (1 + exp(logit)))\n}\n\nlogit_to_prob(coef(intercept_only_model))\n\n\n(Intercept) \n  0.5297297 \n\n\nCode\nlogit_to_prob(confint(intercept_only_model))\n\n\n    2.5 %    97.5 % \n0.5104854 0.5489153 \n\n\nInterpretation: The converted \\(\\beta_0\\) parameter in probability is 0.5297 (95%CI [ 0.5105, 0.5489]). This probability should just be the proportion of mass_trans_spend_right = 1 in the data.\n\nNow let‚Äôs fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n\nCode\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\ndata = data %&gt;%\n  mutate(sex=factor(sex, levels=c(\"Male\", \"Female\")))\n\n#fit with glm()\nm1 &lt;- glm(\n  mass_trans_spend_right ~ 1 + age + sex + sei10,\n  family=binomial,\n  data=data\n  ) \n\n#produce tidy output of model coefficients\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.5697071\n0.1409061\n4.043169\n0.0000527\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n0.2557439\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\n\nConsider the relationship between sex and one‚Äôs opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\n\nCode\n# m1 %&gt;% \n#   tidy() %&gt;%\n#   kable()\n\nm1 %&gt;% \n  tidy(exponentiate = TRUE) %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.7677492\n0.1409061\n4.043169\n0.0000527\n\n\nage\n0.9938530\n0.0022824\n-2.701502\n0.0069027\n\n\nsexFemale\n1.2914219\n0.0798020\n3.204732\n0.0013519\n\n\nsei10\n0.9937922\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nbsex &lt;- coef(m1)[\"sexFemale\"]\nstdsex = m1 %&gt;%\n  tidy %&gt;%\n  filter(`term` == \"sexFemale\") %&gt;%\n  pull(`std.error`)\n\nci_lower_lo = bsex - 1.96 * stdsex\nci_upper_lo = bsex + 1.96 * stdsex\n\nci_lower_or = 1.29 - 1.96 * stdsex\nci_upper_or = 1.29 + 1.96 * stdsex\nprint(paste0(\"Increase in log odds: \", round(bsex,4), \" (95% CI [\", round(ci_lower_lo,4), \" \", round(ci_upper_lo,4), \"])\"))\n\n\n[1] \"Increase in log odds: 0.2557 (95% CI [0.0993 0.4122])\"\n\n\nCode\nprint(paste0(\"Odds ratio: \", round(exp(bsex),4), \" (95% CI [\", round(ci_lower_or,4), \" \", round(ci_upper_or,4), \"])\"))\n\n\n[1] \"Odds ratio: 1.2914 (95% CI [1.1336 1.4464])\"\n\n\nCode\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\nemm_sex\n\n\n sex     prob     SE  df asymp.LCL asymp.UCL\n Male   0.495 0.0147 Inf     0.467     0.524\n Female 0.559 0.0133 Inf     0.533     0.585\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nIf you did this right, you‚Äôll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one‚Äôs opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n\nCode\nbage &lt;- coef(m1)[\"age\"]\nstdage = m1 %&gt;%\n  tidy %&gt;%\n  filter(`term` == \"age\") %&gt;%\n  pull(`std.error`)\n\nci_lower_lo = bage - 1.96 * stdage\nci_upper_lo = bage + 1.96 * stdage\n\nor = exp(bage)\nci_lower_or = exp(bage) - 1.96 * stdage\nci_upper_or = exp(bage) + 1.96 * stdage\n\nprint(paste0(\"Increase in log odds: \", round(bage,4), \" (95% CI [\", round(ci_lower_lo,4), \" \", round(ci_upper_lo,4), \"])\"))\n\n\n[1] \"Increase in log odds: -0.0062 (95% CI [-0.0106 -0.0017])\"\n\n\nCode\nprint(paste0(\"Odds ratio: \", round(or,4), \" (95% CI [\", round(ci_lower_or,4), \" \", round(ci_upper_or,4), \"])\"))\n\n\n[1] \"Odds ratio: 0.9939 (95% CI [0.9894 0.9983])\"\n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.00617 (95% CI [-0.0106, -0.0017]), holding all other variables constant. The odds ratio is 0.993853 (95% CI [0.9894, 0.9983]) which confirms the inverse relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993853, or approximately 0.617% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one‚Äôs opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. √ü\n\n\n\nCode\nbses &lt;- coef(m1)[\"sei10\"]\nstdses = m1 %&gt;%\n  tidy %&gt;%\n  filter(`term` == \"sei10\") %&gt;%\n  pull(`std.error`)\n\nci_lower_lo = bses - 1.96 * stdses\nci_upper_lo = bses + 1.96 * stdses\n\nor = exp(bses)\nci_lower_or = exp(bses) - 1.96 * stdses\nci_upper_or = exp(bses) + 1.96 * stdses\n\nprint(paste0(\"Increase in log odds: \", round(bses,4), \" (95% CI [\", round(ci_lower_lo,4), \" \", round(ci_upper_lo,4), \"])\"))\n\n\n[1] \"Increase in log odds: -0.0062 (95% CI [-0.0095 -0.003])\"\n\n\nCode\nprint(paste0(\"Odds ratio: \", round(or,4), \" (95% CI [\", round(ci_lower_or,4), \" \", round(ci_upper_or,4), \"])\"))\n\n\n[1] \"Odds ratio: 0.9938 (95% CI [0.9905 0.997])\"\n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0095, -0.003]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998])."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#marginal-effects",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#marginal-effects",
    "title": "Lab: Logistic Regression",
    "section": "Marginal effects",
    "text": "Marginal effects\n\nLet‚Äôs examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\n\nCode\navg_comparisons(m1, comparison = \"difference\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ncontrast\nestimate\nstd.error\nstatistic\np.value\ns.value\nconf.low\nconf.high\n\n\n\n\nage\n+1\n-0.0015153\n0.0005579\n-2.716128\n0.0066050\n7.242217\n-0.0026088\n-0.0004219\n\n\nsei10\n+1\n-0.0015304\n0.0004039\n-3.789362\n0.0001510\n12.692835\n-0.0023219\n-0.0007388\n\n\nsex\nFemale - Male\n0.0630688\n0.0196461\n3.210251\n0.0013262\n9.558494\n0.0245632\n0.1015743\n\n\n\n\n\n\nThe marginal effect of age is -0.001515 (95% CI [-0.002609, -0.0004219]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately 0.1515 percentage points, holding other factors constant (p = 0.006605).\nThe marginal effect of SES is -0.001530 (95% CI [-0.002322, -0.0007388]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately 0.1530 percentage points, holding other variables constant (p = 0.0001510).\nThe marginal effect for being female compared to male is 0.06307 (95% CI [0.02456, 0.1016]). This indicates that females are, on average, about 6.3% percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#model-comparison",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#model-comparison",
    "title": "Lab: Logistic Regression",
    "section": "Model comparison",
    "text": "Model comparison\n\nNow let‚Äôs see whether a person‚Äôs political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\n\nCode\nfit2 &lt;- glm(\n  mass_trans_spend_right ~ 1 + age + sex + sei10,\n  family=binomial,\n  data=data\n)\n\nfit3 &lt;- glm(\n  mass_trans_spend_right ~ 1 + age + sex + sei10 + polviews,\n  family=binomial,\n  data=data\n)\n\ntest_likelihoodratio(fit2, fit3) %&gt;% kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#visualization",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#visualization",
    "title": "Lab: Logistic Regression",
    "section": "Visualization",
    "text": "Visualization\n\nLet‚Äôs plot the results\nWe next use the model to produce visualizations:\n\nGiven the code below, interpet what is being plotted:\n\npol_plot : The predicted probability of being satisfied with the spending of mass transportation increases when political views become more conversative, when controlling for age, sex and SES.\nsex_plot : The predicted probability of being satisfied with the spending of mass transportation is higher for women than for men, when controlling for sex, SES, and political views.\nses_plot: The predicted probability of being satisfied with the spending of mass transportation decreases when SES increases, when controlling for age, sex, and political views.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nadjust the various settings in your plot to make it look professional.\nYou can use ggeffects to get the predicted probabilities for these models.\n\n\n\n\n\n\nCode\nlibrary(ggeffects)\n\n\ncolors &lt;- c(\"Extremely liberal\" = \"black\",\n            \"Liberal\" = \"#0e2f44\",  # Dark blue\n            \"Slightly liberal\" = \"#1d5a6c\",  # Less dark blue\n            \"Moderate\" = \"#358ca3\",  # Medium blue\n            \"Slghtly conservative\" = \"#71b9d1\",  # Light blue\n            \"Conservative\" = \"#a6dcef\",  # Lighter blue\n            \"Extrmly conservative\" = \"#d0f0fd\")  # Very light blue\n\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\npp_pol\n\n\n# Predicted probabilities of mass_trans_spend_right\n\npolviews             | Predicted |     95% CI\n---------------------------------------------\nExtremely liberal    |      0.34 | 0.27, 0.43\nLiberal              |      0.39 | 0.34, 0.45\nSlightly liberal     |      0.49 | 0.43, 0.54\nModerate             |      0.57 | 0.53, 0.60\nSlghtly conservative |      0.55 | 0.50, 0.60\nConservative         |      0.58 | 0.53, 0.63\nExtrmly conservative |      0.66 | 0.57, 0.75\n\nAdjusted for:\n*   age = 48.90\n* sei10 = 46.07\n\n\nCode\n# Adjusted plot with gradient colors\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = colors) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + \n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  theme_minimal()\npol_plot\n\n\n\n\n\n\n\n\n\nCode\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\npp_sex\n\n\n# Predicted probabilities of mass_trans_spend_right\n\nsex    | Predicted |     95% CI\n-------------------------------\nMale   |      0.48 | 0.44, 0.51\nFemale |      0.55 | 0.51, 0.58\n\nAdjusted for:\n*   age = 48.90\n* sei10 = 46.07\n\n\nCode\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  theme_minimal()\n\nsex_plot\n\n\n\n\n\n\n\n\n\nCode\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\npp_ses\n\n\n# Predicted probabilities of mass_trans_spend_right\n\nsei10 | Predicted |     95% CI\n------------------------------\n    0 |      0.57 | 0.52, 0.61\n   15 |      0.55 | 0.51, 0.58\n   25 |      0.54 | 0.51, 0.57\n   35 |      0.52 | 0.50, 0.55\n   50 |      0.51 | 0.48, 0.53\n   65 |      0.49 | 0.46, 0.52\n   75 |      0.48 | 0.44, 0.51\n  100 |      0.45 | 0.40, 0.50\n\nAdjusted for:\n* age = 48.90\n\n\nCode\nses_plot &lt;-  ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"#2c7fb8\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#2c7fb8\", alpha = 0.2) +  # Add a confidence interval band\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \nses_plot"
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#model-assumptions",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#model-assumptions",
    "title": "Lab: Logistic Regression",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nIs the logistic model a good choice for this data?\n\n\n\nCode\nbinned_residuals(fit2)\n\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\nCode\nbinned_residuals(fit2) %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: It seems like the model does not fit the data very well. About 86% of the residuals are inside the error bounds, as opposed to the ideal &gt;= 95%."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#model-fit",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#model-fit",
    "title": "Lab: Logistic Regression",
    "section": "Model fit",
    "text": "Model fit\n\nCalculate the \\(R^2\\) for this model\n\n\n\nCode\nr2_mcfadden(fit2)\n\n\n# R2 for Generalized Linear Regression\n       R2: 0.010\n  adj. R2: 0.009\n\n\n\nR2 interpretation: The model explains around 1% of the variance in the dependent variable, which means it is not a good fit.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\")\n\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\nCode\nbinned_residuals(fit2, term=\"age\")\n\n\nOk: About 98% of the residuals are inside the error bounds.\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\nCode\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere SES seems to stick out. For SES, only 88% of the residuals are inside the error bounds while for age, about 98% of the residuals are inside the error bounds. However, for the binned residual plot, it looks like there is no systematic trends, so I think it‚Äôs reasonable to assume linearity. If we want, we can transform SES or add interaction terms to improve model fit."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#testing-polviews",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#testing-polviews",
    "title": "Lab: Logistic Regression",
    "section": "Testing Polviews",
    "text": "Testing Polviews\n\n\nCode\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\n\nCode\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 2.7008 and 2.1726 times more likely to support mass transit spending compared to extremely liberal and liberal\nExtreme liberals are 0.3703, 0.3959, and 0.4280 times more likely to support spending compared to conservatives, moderates and slight conservatives\nExtrm conservatives are 3.8200 and 2.0821 times more likely to support mass spending than liberals and slight liberals\nLiberals are 0.4921 and 0.5320 times more likely to support spending than moderates and slight conservatives."
  },
  {
    "objectID": "posts/02-12 logistic/Lab-Logistic-Q.html#conclusion",
    "href": "posts/02-12 logistic/Lab-Logistic-Q.html#conclusion",
    "title": "Lab: Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the model summary below, and the three figures, we conclude that age, sex, SES, and political views are all significant variables in predicting satisfaction of mass transit spending. Specifically, people that are younger tend to be more satisfied with mass transit spending; females are more satisfied with mass transit spending than males in general; people of lower socioeconomic status are more satisfied with mass transit spending; and people with more conservative political views are more satisfied with mass transit spending.\n\n\n\n\nDf\nDeviance\nResid. Df\nResid. Dev\nPr(&gt;Chi)\n\n\n\n\nNULL\nNA\nNA\n2589\n3581.340\nNA\n\n\nage\n1\n9.268443\n2588\n3572.072\n0.0023314\n\n\nsex\n1\n12.156624\n2587\n3559.915\n0.0004891\n\n\nsei10\n1\n14.119078\n2586\n3545.796\n0.0001716\n\n\npolviews\n6\n63.028441\n2580\n3482.768\n0.0000000\n\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html",
    "href": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html",
    "title": "Lab: Multinomial Regression",
    "section": "",
    "text": "Lab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight ‚ÄúWhy Many Americans Don‚Äôt Vote‚Äù survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#data",
    "href": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#data",
    "title": "Lab: Multinomial Regression",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article ‚ÄúWhy Many Americans Don‚Äôt Vote‚Äù. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone‚Äôs party identification to understand whether a person is a probable voter.\nThe variables we‚Äôll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question ‚ÄúGenerally speaking, do you think of yourself as a‚Ä¶‚Äù\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\n\nCode\nlibrary(nnet)\nlibrary(car)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(parameters)\nlibrary(easystats)\n\n\n\n\nCode\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")"
  },
  {
    "objectID": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#lrt",
    "href": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#lrt",
    "title": "Lab: Multinomial Regression",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\nCode\ncar::Anova(model2) %&gt;% \n  kable()\n\n\n\n\n\n\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nppage\n638.297213\n2\n0.000000\n\n\nrace\n52.651508\n6\n0.000000\n\n\ngender\n6.027914\n2\n0.049097\n\n\nincome_cat\n67.721466\n6\n0.000000\n\n\neduc\n154.136763\n4\n0.000000\n\n\npol_ident_new\n153.843978\n6\n0.000000\n\n\n\n\n\n\nAll the terms: age, race, gender, income, education, and party identification are all significant in predicting voter category"
  },
  {
    "objectID": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#marginal-effects-political-group---emmeans",
    "href": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#marginal-effects-political-group---emmeans",
    "title": "Lab: Multinomial Regression",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n\nCode\n#Get estimated marginal means from the model\n\n#using \nmultinomial_analysis &lt;- emmeans(model2, ~ pol_ident_new|voter_category)\n\ncoefs = contrast(regrid(multinomial_analysis, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\") %&gt;%\n kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004"
  },
  {
    "objectID": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#marginal-effects-of-education---emmeans",
    "href": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#marginal-effects-of-education---emmeans",
    "title": "Lab: Multinomial Regression",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n\nCode\n#Get estimated marginal means from the model\n\n#using \nmultinomial_analysis &lt;- emmeans(model2, ~ educ|voter_category)\n\ncoefs = contrast(regrid(multinomial_analysis, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\") %&gt;%\n kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\n\nCode\n  ggemmeans(model2, terms = c(\"ppage\")) %&gt;% \n      ggplot(., aes(x = x, y = predicted, fill = response.level)) +\n      geom_area() + \n      geom_rug(sides = \"b\", position = \"jitter\", alpha = .5) + \n      labs(x = \"\\nAge\", y = \"Predicted Probablity\\n\", title = \"Predicted Probabilities of Voting Frequency by Age\") +\n      scale_fill_manual(\n        name = NULL,\n        values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n        labels = c(\"RARELY OR NEVER VOTE    \", \"SOMETIMES VOTE    \", \"ALMOST ALWAYS VOTE    \"),\n        breaks = c(\"rarely/never\", \"sporadic\", \"always\")\n      ) +\n      theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n ggemmeans(model2, terms=c(\"pol_ident_new\")) %&gt;%   ggplot(., aes(x = x, y = predicted, fill = response.level)) + \n  geom_bar(stat = \"identity\" ) +\n    geom_text(aes(label = round(predicted, 3)), color=\"white\", position = position_fill(vjust = 0.5),size=3)  + \n  labs(x=\"Education\", y=\"Predicted Probablity\") + \n  theme(text = element_text(size = 30)) +  \n  scale_fill_viridis(discrete = TRUE) + \n  theme_lucid(base_size=25)\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\n\nCode\n ggemmeans(model2, terms=c(\"educ\")) %&gt;% ggplot(., aes(x = x, y = predicted, fill = response.level)) + \n  geom_bar(stat = \"identity\" ) +\n    geom_text(aes(label = round(predicted, 3)), color=\"white\", position = position_fill(vjust = 0.5),size=3)  + \n  labs(x=\"Education\", y=\"Predicted Probablity\") + \n  theme(text = element_text(size = 30)) +  \n  scale_fill_viridis(discrete = TRUE) + \n  theme_lucid(base_size=25)"
  },
  {
    "objectID": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#write-up",
    "href": "posts/02-26 multinomial/Lab4_multinom_Questions-1.html#write-up",
    "title": "Lab: Multinomial Regression",
    "section": "Write-up",
    "text": "Write-up\n\nDifferences between political groups and voting behavior - Emmeans\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model2, ~ pol_ident_new|voter_category)\n\ncoefs = contrast(regrid(multinomial_analysis, \"log\"),\"trt.vs.ctrl1\",  by=\"pol_ident_new\")\n# you can add a parameter to the above command, ref = newbaseline, if you want to change baseline\n\nupdate(coefs, by = \"contrast\") %&gt;%\n kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n\nCode\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\nEnter your interpretation here:\nVoters who are Democrats are 2.61 times more likely to vote sporadically than vote rarely/never.\nVoters who are Democrats are 1.62 times more likely to vote always than vote rarely/never.\nVoters who are Independents are 1.81 times more likely to vote sporadically than vote rarely/never.\nVoters who are Independents are 4.78% less likely to vote always than vote rarely/never.\nVoters who belong to other political parties are 1.08 times more likely to vote sporadically than vote rarely/never.\nVoters who belong to other political parties are 56.61% less likely to vote always than vote rarely/never.\nVoters who are Republicans are 2.42 times more likely to vote sporadically than vote rarely/never.\nVoters who are Republicans are 1.39 times more likely to vote always than vote rarely/never.\nVoters who are Independents are 30.93% less likely to vote sporadically than vote rarely/never compared to voters who are Democrats.\nVoters who belong to voters who belong to other political parties are 58.65% less likely to vote sporadically than vote rarely/never compared to voters who are Democrats.\nVoters who belong to other political parties are 40.13% less likely to vote sporadically than vote rarely/never compared to voters who are Republicans.\nVoters who are Republicans are 7.50% less likely to vote sporadically than vote rarely/never compared to voters who are Democrats.\nVoters who are Republicans are 1.34 times more likely to vote sporadically than vote rarely/never compared to voters who are Independents.\nVoters who are Republicans are 2.24 times more likely to vote sporadically than vote rarely/never compared to voters who belong to other political parties.\nVoters who are Independents are 41.08% less likely to vote always than vote rarely/never compared to voters who are Democrats.\nVoters who belong to other political parties are 73.15% less likely to vote always than vote rarely/never compared to voters who are Democrats.\nVoters who belong to other political parties are 54.43% less likely to vote always than vote rarely/never compared to voters who are Republicans.\nVoters who are Republicans are 14.19% less likely to vote always than vote rarely/never compared to voters who are Democrats.\nVoters who are Republicans are 1.46 times more likely to vote always than vote rarely/never compared to voters who are Independents.\nVoters who are Republicans are 3.20 times more likely to vote always than vote rarely/never compared to voters who belong to other political parties.\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\n\nCode\nmulti_an &lt;- emmeans(model, ~ educ|voter_category)\n\ncoefs = contrast(regrid(multi_an, \"log\"),\"trt.vs.ctrl1\",  by=\"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n1.156\n0.075\n22\n15.483\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.711\n0.079\n22\n9.041\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.259\n0.069\n22\n3.749\n0.003\n\n\nalways - (rarely/never)\nHigh school or less\n-0.602\n0.081\n22\n-7.476\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.806\n0.075\n22\n10.818\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.311\n0.080\n22\n3.895\n0.002\n\n\n\n\n\nCode\n# get difference between yes-no and fair-excellent\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.897\n0.095\n22\n-9.409\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.349\n0.092\n22\n-3.779\n0.003\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.547\n0.089\n22\n6.162\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.313\n0.105\n22\n-12.463\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.401\n0.098\n22\n-4.091\n0.001\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.913\n0.099\n22\n9.206\n0.000\n\n\n\n\n\nEnter your interpretation here:\nVoters with a highest degree of college are 3.18 times more likely to vote sporadically than vote rarely/never.\nVoters with a highest degree of college are 2.04 times more likely to vote always than vote rarely/never.\nVoters with a highest degree of high school or less are 1.30 times more likely to vote sporadically than vote rarely/never.\nVoters with a highest degree of high school or less are 45.23% less likely to vote always than vote rarely/never.\nVoters with a highest degree of some college are 2.24 times more likely to vote sporadically than vote rarely/never.\nVoters with a highest degree of some college are 1.36 times more likely to vote always than vote rarely/never.\nVoters with a highest degree of high school or less are 59.22% less likely to vote sporadically than vote rarely/never compared to voters with a highest degree of college.\nVoters with a highest degree of some college are 29.46% less likely to vote sporadically than vote rarely/never compared to voters with a highest degree of college.\nVoters with a highest degree of some college are 1.73 times more likely to vote sporadically than vote rarely/never compared to voters with a highest degree of high school or less.\nVoters with a highest degree of high school or less are 73.10% less likely to vote always than vote rarely/never compared to voters with a highest degree of college.\nVoters with a highest degree of some college are 33.03% less likely to vote always than vote rarely/never compared to voters with a highest degree of college.\nVoters with a highest degree of some college are 2.49 times more likely to vote always than vote rarely/never compared to voters with a highest degree of high school or less."
  },
  {
    "objectID": "posts/02-19 ordinal/ord_lab_q.html",
    "href": "posts/02-19 ordinal/ord_lab_q.html",
    "title": "Lab: Ordinal Regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don‚Äôt forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you‚Äôre done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "posts/02-19 ordinal/ord_lab_q.html#instructions",
    "href": "posts/02-19 ordinal/ord_lab_q.html#instructions",
    "title": "Lab: Ordinal Regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don‚Äôt forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you‚Äôre done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "posts/02-19 ordinal/ord_lab_q.html#load-packages",
    "href": "posts/02-19 ordinal/ord_lab_q.html#load-packages",
    "title": "Lab: Ordinal Regression",
    "section": "Load packages:",
    "text": "Load packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(performance)\nlibrary(ordinal) #clm\nlibrary(car) # anova\nlibrary(ggeffects) #  viz\nlibrary(gofcat) # brant\nlibrary(brms)\nlibrary(emmeans) # contrasts\nlibrary(knitr)"
  },
  {
    "objectID": "posts/02-19 ordinal/ord_lab_q.html#load-data",
    "href": "posts/02-19 ordinal/ord_lab_q.html#load-data",
    "title": "Lab: Ordinal Regression",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\n\nCode\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\ngb &lt;- gbbo %&gt;%\n  filter(`Technical Rank` &lt;= 3) %&gt;%\n  mutate(`Technical Rank` = factor(`Technical Rank`, levels=c(1,2,3), ordered=TRUE),\n         Gender = factor(Gender, levels=c(\"M\",\"F\"), ordered=TRUE))"
  },
  {
    "objectID": "posts/02-19 ordinal/ord_lab_q.html#explore",
    "href": "posts/02-19 ordinal/ord_lab_q.html#explore",
    "title": "Lab: Ordinal Regression",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank‚Äî create one for Gender and Age\n\n\n\nCode\ngb_gender &lt;- gb %&gt;%\n  count(Gender, `Technical Rank`) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(gb_gender, aes(x = Gender, y = proportion, fill = `Technical Rank`)) +\n  geom_bar(stat='identity', position='fill') + \n  labs(x=\"Gender\", y=\"Proportion of Technical Rank\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ngb_age &lt;- gb %&gt;%\n  mutate(Age = cut_number(Age,8)) %&gt;%\n  count(Age, `Technical Rank`) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\nggplot(gb_age, aes(x = Age, y = proportion, fill = `Technical Rank`)) +\n  geom_bar(stat='identity', position='fill') + \n  labs(x=\"Age (binned)\", y=\"Proportion of Technical Rank\")"
  },
  {
    "objectID": "posts/02-19 ordinal/ord_lab_q.html#ordinal-analysis",
    "href": "posts/02-19 ordinal/ord_lab_q.html#ordinal-analysis",
    "title": "Lab: Ordinal Regression",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven‚Äôt already, convert the outcome variable to an ordered factor. What does the order here represent?\nThe order here represent the technical rank: first, second, third.\nConvert input variables to categorical factors as appropriate.\n\n\nCode\nstr(gb)\n\n\ntibble [309 √ó 3] (S3: tbl_df/tbl/data.frame)\n $ Gender        : Ord.factor w/ 2 levels \"M\"&lt;\"F\": 2 1 1 2 1 2 1 2 2 1 ...\n $ Age           : num [1:309] 30 31 24 45 25 37 24 37 31 24 ...\n $ Technical Rank: Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 2 3 1 2 1 3 1 3 2 3 ...\n\n\nCode\ngb = gb %&gt;%\n  mutate(Technical_Rank = `Technical Rank`)\n\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\nCode\nmodel1 = clm(Technical_Rank~1 + Gender + Age, data=gb, link=\"logit\")\nmodel2 = clm(Technical_Rank~1 + Gender + Age + Gender * Age, data=gb, link=\"logit\")\n# summary(model2)\nmodel2 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\ncoef.type\n\n\n\n\n1|2\n-0.8419558\n0.3422080\n-2.460363\n0.0138797\nintercept\n\n\n2|3\n0.5796803\n0.3404226\n1.702826\n0.0886007\nintercept\n\n\nGender.L\n0.8127997\n0.4758129\n1.708234\n0.0875930\nlocation\n\n\nAge\n-0.0037139\n0.0092343\n-0.402186\n0.6875471\nlocation\n\n\nGender.L:Age\n-0.0274268\n0.0131038\n-2.093039\n0.0363456\nlocation\n\n\n\n\n\nCode\nprint(exp(0.8127997))\n\n\n[1] 2.25421\n\n\nCode\nprint(1-exp(-0.0037139))\n\n\n[1] 0.003707012\n\n\nCode\nprint(1-exp(-0.0274268))\n\n\n[1] 0.0270541\n\n\nGender: The odds of being in a higher Technical_Rank category are 2.25 times greater for Females compared to Males, controlling for Age and the interaction between Gender and Age.\nAge: For every one year increase in age, the odds of being in a higher Technical_Rank category decreases by approximately 0.37%, controlling for Gender and the interaction between Gender and Age.\nGender and Age: For each additional year increase in Age, the odds of being in a higher Technical_Rank category decreases by about 2.71% more for Females than for Males, controlling for Gender and Age.\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n\n\nCode\n    anova_test &lt;- anova(model1, model2)\n    anova_test\n\n\nLikelihood ratio tests of cumulative link models:\n \n       formula:                                         link: threshold:\nmodel1 Technical_Rank ~ 1 + Gender + Age                logit flexible  \nmodel2 Technical_Rank ~ 1 + Gender + Age + Gender * Age logit flexible  \n\n       no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel1      4 685.72 -338.86                        \nmodel2      5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYes, the interaction is warranted.\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\nCode\n# print(ggemmeans(model2, terms=c(\"Age\",\"Gender\")), n = Inf)\n\nplot(ggemmeans(model2, terms=c(\"Gender\",\"Age\"))) +\n  labs(title = \"Predicted Probabilities of Technical Rank\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggemmeans(model2, terms=c(\"Age [all]\",\"Gender\"))) +\n  labs(title = \"Predicted Probabilities of Technical Rank\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\n\nCode\nols_clm = MASS::polr(Technical_Rank~Gender*Age, data=gb)\n\nggeffect(ols_clm, c(\"Age[all]\", \"Gender\"), latent=TRUE) %&gt;% plot()\n\n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nCode\nbrant.test(ols_clm)\n\n\n\nBrant Test:\n                chi-sq   df   pr(&gt;chi)\nOmnibus          1.295    3       0.73\nGender.L         0.585    1       0.44\nAge              0.415    1       0.52\nGender.L:Age     0.924    1       0.34\n\nH0: Proportional odds assumption holds\n\n\nThe proportional odds assumption holds since all p-vlaues are &gt; 0.05.\nbrms\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place. With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\n\nCode\n  ols2_brm = brm(Technical_Rank ~  Gender*Age, data=gb, family = cumulative, cores = 4,chains = 4)\n\n\nThe `brm` package uses a bayesian approach, estimates parameters using Markov Chain Mote Carlo (MCMC), while the `clm` package uses a frequentist approach, estimates parameters via maximum likelihood estimation (MLE). `clm` is much faster and deterministic, while `brms` is slower and stochastic.\n\nThe results from `brms` provide posterior distributions with credible intervals, which tend to be wider than the standard errors in `clm`, reflecting greater uncertainty. While the point estimates from both models are similar, the bayesian approach allows for probabilistic statements about the parameters. The posterior distributions directly tell you the probability of a parameter being within a given range. Thus, I would say `brms` is more flexible and interpretable, while `clm` is computationally efficient and useful for quick estimation.\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n\n\nCode\nconditional_effects(ols2_brm, categorical = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis makes sense. Higher probabilities for higher Technical_Rank for females and younger ages.\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?). Run the below code. What do you think?\n\n\n\nCode\ncheck_predictions(ols2_brm)\n\n\n\n\n\n\n\n\n\nI would say it fits the data pretty well. The observed data is within the model-predicted data intervals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Labs & Assignments",
    "section": "",
    "text": "Lab: Poisson Regression\n\n\nPrinceton University\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nKW\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Multinomial Regression\n\n\nPrinceton University\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nKW\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Ordinal Regression\n\n\nPrinceton University\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\nKW\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Logistic Regression\n\n\nPrinceton University\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 12, 2025\n\n\nKW\n\n\n\n\n\n\n\n\n\n\n\n\nMillie\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\nKW\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html",
    "title": "Lab: Poisson Regression",
    "section": "",
    "text": "To complete this lab:\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(easystats)\nlibrary(performance)\nlibrary(knitr)\nCode\nlibrary(tidyverse)\n\ndata &lt;- read_delim(\"https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/2010.csv\")\nCode\nlibrary(naniar)\n\ndata_pos &lt;- data %&gt;%\n  dplyr::select(wwwhr, wordsum, age, sex, reliten, polviews, wrkhome) %&gt;%\nreplace_with_na(.,\n             replace = list(wwwhr = c(-1, 998, 999),\n                          wordsum = c(-1, 99),\n                          reliten = c(0, 8, 9), \n             polviews = c(0, 8, 9), \n             wrkhome = c(0,8,9), \n             age=c(0, 98, 99)))\nQ: Can you explain what might be going on in the above code?\nA: The code is using dplyr package to select 7 columns, then replace the values specified in the list with NAs. For instance, it replaces wwwhr values -1, 998, and 999 to NAs.\nQ: The next step in data cleaning would be to ensure that the data in your code are aligned with the description/ usage context of the variables\nCode\ndata_pos = data_pos %&gt;%\n  mutate(age_recode = age - mean(age, na.rm=TRUE),\n         sex_recode = as.factor(sex),\n         reliten_recode = as.factor(reliten),\n         polviews_recode = as.factor(polviews),\n         wrkhome_recode = as.factor(wrkhome))"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#missingness",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#missingness",
    "title": "Lab: Poisson Regression",
    "section": "Missingness",
    "text": "Missingness\n\n\nCode\ndata_pos %&gt;%\n  dplyr::select(reliten, reliten_recode)\n\n\n# A tibble: 2,044 √ó 2\n   reliten reliten_recode\n     &lt;dbl&gt; &lt;fct&gt;         \n 1       1 1             \n 2       4 4             \n 3       1 1             \n 4       1 1             \n 5       1 1             \n 6       4 4             \n 7       3 3             \n 8       1 1             \n 9       1 1             \n10       1 1             \n# ‚Ñπ 2,034 more rows\n\n\nCode\nlibrary(skimr)\nskimr::skim(data_pos)\n\n\n\n\n\n\nName\ndata_pos\n\n\nNumber of rows\n2044\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex_recode\n0\n1.00\nFALSE\n2\n-1: 1153, 1: 891\n\n\nreliten_recode\n99\n0.95\nFALSE\n4\n2: 747, 1: 707, 4: 363, 3: 128\n\n\npolviews_recode\n71\n0.97\nFALSE\n7\n4: 746, 6: 315, 5: 265, 2: 259\n\n\nwrkhome_recode\n882\n0.57\nFALSE\n6\n1: 674, 5: 147, 2: 101, 4: 92\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwwhr\n996\n0.51\n9.79\n13.41\n0.00\n2.00\n5.00\n14.00\n168.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nwordsum\n657\n0.68\n6.03\n2.07\n0.00\n5.00\n6.00\n7.00\n10.00\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÇ\n\n\nage\n3\n1.00\n47.97\n17.68\n18.00\n33.00\n47.00\n61.00\n89.00\n‚ñá‚ñá‚ñá‚ñÖ‚ñÉ\n\n\nsex\n0\n1.00\n-0.13\n0.99\n-1.00\n-1.00\n-1.00\n1.00\n1.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÜ\n\n\nreliten\n99\n0.95\n2.08\n1.08\n1.00\n1.00\n2.00\n3.00\n4.00\n‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ\n\n\npolviews\n71\n0.97\n4.08\n1.46\n1.00\n3.00\n4.00\n5.00\n7.00\n‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÖ\n\n\nwrkhome\n882\n0.57\n2.26\n1.72\n1.00\n1.00\n1.00\n4.00\n6.00\n‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÅ\n\n\nage_recode\n3\n1.00\n0.00\n17.68\n-29.97\n-14.97\n-0.97\n13.03\n41.03\n‚ñá‚ñá‚ñá‚ñÖ‚ñÉ"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#fit-a-poisson-model-to-the-data.",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#fit-a-poisson-model-to-the-data.",
    "title": "Lab: Poisson Regression",
    "section": "Fit a Poisson model to the data.",
    "text": "Fit a Poisson model to the data.\n\n\nCode\nlibrary(lme4)\nmodel1 = glm(wwwhr~age_recode+wordsum+sex_recode+reliten_recode+polviews_recode+wrkhome_recode, \n              data=data_pos,\n              family=poisson(link = \"log\"))"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#carry-out-model-checking",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#carry-out-model-checking",
    "title": "Lab: Poisson Regression",
    "section": "Carry out model checking",
    "text": "Carry out model checking\nHint: performance package has the function you‚Äôre looking for\n\n\nCode\nlibrary(performance)\nperformance::check_model(model1, check = c(\"pp_check\", \"outliers\", \"vif\", \"overdispersion\"))"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#find-any-outliers",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#find-any-outliers",
    "title": "Lab: Poisson Regression",
    "section": "Find any outliers",
    "text": "Find any outliers\n\n\nCode\noutlier_idx = check_outliers(model1)\noutlier_idx\n\n\n3 outliers detected: cases 72, 156, 363.\n- Based on the following method and threshold: cook (0.8).\n- For variable: (Whole model)."
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#refit-the-model-after-excluding-outliers",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#refit-the-model-after-excluding-outliers",
    "title": "Lab: Poisson Regression",
    "section": "Refit the model after excluding outliers",
    "text": "Refit the model after excluding outliers\n\n\nCode\ndata_pos2 = data_pos %&gt;%\n  filter(! row_number() %in% which(outlier_idx))\n\nmodel2 = glm(wwwhr~age+wordsum+sex_recode+reliten_recode+polviews_recode+wrkhome_recode, \n              data=data_pos2,\n              family=poisson(link=\"log\"))\n\n\n\n\nCode\nmodel_parameters(model2) %&gt;%\n  print_html()\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.89\n0.09\n(1.71, 2.07)\n20.52\n&lt; .001\n\n\nage\n-0.02\n1.10e-03\n(-0.02, -0.01)\n-15.29\n&lt; .001\n\n\nwordsum\n0.10\n7.86e-03\n(0.08, 0.11)\n12.38\n&lt; .001\n\n\nsex recode (1)\n0.22\n0.03\n(0.17, 0.27)\n8.13\n&lt; .001\n\n\nreliten recode (2)\n0.34\n0.04\n(0.26, 0.41)\n8.88\n&lt; .001\n\n\nreliten recode (3)\n0.43\n0.06\n(0.30, 0.55)\n6.68\n&lt; .001\n\n\nreliten recode (4)\n0.65\n0.04\n(0.57, 0.72)\n16.23\n&lt; .001\n\n\npolviews recode (2)\n-0.10\n0.07\n(-0.23, 0.03)\n-1.49\n0.136\n\n\npolviews recode (3)\n-0.18\n0.07\n(-0.31, -0.04)\n-2.51\n0.012\n\n\npolviews recode (4)\n-0.21\n0.06\n(-0.33, -0.08)\n-3.23\n0.001\n\n\npolviews recode (5)\n-0.06\n0.07\n(-0.19, 0.07)\n-0.92\n0.359\n\n\npolviews recode (6)\n-0.25\n0.07\n(-0.39, -0.11)\n-3.46\n&lt; .001\n\n\npolviews recode (7)\n-0.31\n0.10\n(-0.51, -0.11)\n-3.02\n0.003\n\n\nwrkhome recode (2)\n0.21\n0.04\n(0.12, 0.29)\n4.57\n&lt; .001\n\n\nwrkhome recode (3)\n0.35\n0.05\n(0.26, 0.44)\n7.50\n&lt; .001\n\n\nwrkhome recode (4)\n0.44\n0.04\n(0.35, 0.52)\n9.76\n&lt; .001\n\n\nwrkhome recode (5)\n0.24\n0.04\n(0.15, 0.32)\n5.64\n&lt; .001\n\n\nwrkhome recode (6)\n0.40\n0.06\n(0.29, 0.51)\n7.20\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck for Overdispersion\nHint: performance package has the function you‚Äôre looking for\n\n\nCode\nperformance::check_overdispersion(model2)\n\n\n# Overdispersion test\n\n       dispersion ratio =   15.102\n  Pearson's Chi-Squared = 8804.224\n                p-value =  &lt; 0.001\n\n\nWhat do you notice? And what‚Äôs a good next step forward? Can there be another model class that can fit the data? If so, fit this model to the data.\n\nThere is overdispersion, which means there is more variation in the response than what‚Äôs implied by a Poisson model. We can try to fit a negative-binomial regression model.\n\n\n\nCode\nmodel3 = glm.nb(wwwhr~age+wordsum+sex_recode+reliten_recode+polviews_recode+wrkhome_recode, \n              data=data_pos2)\n\nmodel4 = MASS::glm.nb(wwwhr~age+wordsum+sex_recode+reliten_recode+polviews_recode+wrkhome_recode, \n              data=data_pos2)"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#which-one-is-better--your-earlier-model-or-later-model",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#which-one-is-better--your-earlier-model-or-later-model",
    "title": "Lab: Poisson Regression",
    "section": "Which one is better- your earlier model, or later model?",
    "text": "Which one is better- your earlier model, or later model?\n\n\nCode\ntest_likelihoodratio(model2, model3) %&gt;%\n  kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nmodel2\nmodel2\nglm\n18\nNA\nNA\nNA\n\n\nmodel3\nmodel3\nnegbin\n19\n1\n4510.038\n0\n\n\n\n\n\nCode\ntest_likelihoodratio(model2, model4) %&gt;%\n  kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nmodel2\nmodel2\nglm\n18\nNA\nNA\nNA\n\n\nmodel4\nmodel4\nnegbin\n19\n1\n4510.038\n0\n\n\n\n\n\nThe later model is better here, which means the previous poisson model was not a good fit to the data."
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "title": "Lab: Poisson Regression",
    "section": "What is zero inflation? Is there zero-inflation in your chosen model?",
    "text": "What is zero inflation? Is there zero-inflation in your chosen model?\n\n\nCode\nperformance::check_zeroinflation(model3)\n\n\n# Check for zero-inflation\n\n   Observed zeros: 40\n  Predicted zeros: 67\n            Ratio: 1.68\n\n\nThere is no zero-inflation here since # of observed zeros &lt; # of predicted zeros.\n\nLog LambdaMean Count\n\n\n\n\nCode\nprint(coef(model4))\n\n\n     (Intercept)              age          wordsum      sex_recode1 \n      1.72933133      -0.01630350       0.10573220       0.14429163 \n reliten_recode2  reliten_recode3  reliten_recode4 polviews_recode2 \n      0.28114285       0.38546776       0.62578244       0.17573144 \npolviews_recode3 polviews_recode4 polviews_recode5 polviews_recode6 \n      0.09431515      -0.04193018       0.13914802      -0.04427420 \npolviews_recode7  wrkhome_recode2  wrkhome_recode3  wrkhome_recode4 \n     -0.15510946       0.11140958       0.31377893       0.30530537 \n wrkhome_recode5  wrkhome_recode6 \n      0.15257360       0.25947760 \n\n\nCode\nprint(exp(coef(model4)))\n\n\n     (Intercept)              age          wordsum      sex_recode1 \n       5.6368834        0.9838287        1.1115242        1.1552210 \n reliten_recode2  reliten_recode3  reliten_recode4 polviews_recode2 \n       1.3246428        1.4703019        1.8697083        1.1921179 \npolviews_recode3 polviews_recode4 polviews_recode5 polviews_recode6 \n       1.0989060        0.9589367        1.1492942        0.9566916 \npolviews_recode7  wrkhome_recode2  wrkhome_recode3  wrkhome_recode4 \n       0.8563214        1.1178527        1.3685871        1.3570393 \n wrkhome_recode5  wrkhome_recode6 \n       1.1648282        1.2962527 \n\n\nCode\nmean(exp(predict(model4, type = \"link\")))\n\n\n[1] 9.879506\n\n\n\n\n\n\nCode\nprint(mean(data_pos2$wwwhr, na.rm = TRUE))\n\n\n[1] 9.792543\n\n\nCode\ndata_pos_base = data_pos2 %&gt;%\n  filter(sex_recode==-1, reliten_recode==1, polviews_recode==1, wrkhome_recode==1)\nmean(data_pos_base$wwwhr, na.rm = TRUE)\n\n\n[1] 5.4"
  },
  {
    "objectID": "posts/03-05 poisson/poisson_lab_questions-1.html#report-your-conclusions",
    "href": "posts/03-05 poisson/poisson_lab_questions-1.html#report-your-conclusions",
    "title": "Lab: Poisson Regression",
    "section": "Report your conclusions",
    "text": "Report your conclusions\nThe coefficients of the model is roughly similar to the log value of the mean of the dependent variable. The exponential of the intercept of the model is 5.637, while the mean number of hours per week that a person spends on the internet (wwwhr) for the baseline group (sex=-1, religosity=1, political_orientation=1, work_from_home=1) is 5.4.\nBecause of the numerous number of levels for different categorical variables, here we don‚Äôt look at each level. We can use our full model to predict the dependent variable, and then take the mean of the exponential, which is 9.880, while the mean wwwhr for the whole dataset is 9.793.\nOverall, a negative-binomial regression model is a good fit to the data due to dispersion. We don‚Äôt need to use a zero-inflated model since there is no zero-inflation."
  }
]