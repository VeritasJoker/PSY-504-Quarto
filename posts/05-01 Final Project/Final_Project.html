<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KW">
<meta name="dcterms.date" content="2025-05-03">

<title>Final Project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Final_Project_files/libs/clipboard/clipboard.min.js"></script>
<script src="Final_Project_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="Final_Project_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="Final_Project_files/libs/quarto-html/popper.min.js"></script>
<script src="Final_Project_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Final_Project_files/libs/quarto-html/anchor.min.js"></script>
<link href="Final_Project_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Final_Project_files/libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Final_Project_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Final_Project_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Final_Project_files/libs/bootstrap/bootstrap-83837c041b80f026fb76b1201b2b9e7e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#loading-packages-dataset" id="toc-loading-packages-dataset" class="nav-link active" data-scroll-target="#loading-packages-dataset">Loading Packages &amp; Dataset</a></li>
  <li><a href="#eda" id="toc-eda" class="nav-link" data-scroll-target="#eda">EDA</a></li>
  <li><a href="#embedding-extraction" id="toc-embedding-extraction" class="nav-link" data-scroll-target="#embedding-extraction">Embedding Extraction</a></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification">Classification</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="../../../../../scratch/gpfs/kw1166/247/05-01 Final Project/Final_Project.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Final Project</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">Princeton University</p>
  <div class="quarto-categories">
    <div class="quarto-category">code</div>
    <div class="quarto-category">analysis</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>KW </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 3, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<hr>
<p>This is a tutorial to perform binary classifications for the <a href="https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews/data">Amazon Reviews</a> dataset.</p>
<p>The Amazon reviews dataset consists of reviews from Amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. Here, we take review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 are ignored.</p>
<section id="loading-packages-dataset" class="level2">
<h2 class="anchored" data-anchor-id="loading-packages-dataset">Loading Packages &amp; Dataset</h2>
<div id="2c1e8029" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># os</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># nlp</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># classification</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier, RandomForestClassifier</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.colors <span class="im">as</span> mcolors</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="748e1ed7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">"train.csv"</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">"test.csv"</span>, header<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="eda" class="level2">
<h2 class="anchored" data-anchor-id="eda">EDA</h2>
<div id="8099f05a" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Length of the data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(train_df))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(test_df))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter out NaNs</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>train_df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>test_df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Name the columns</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>train_df.columns <span class="op">=</span> [<span class="st">"rating"</span>, <span class="st">"title"</span>, <span class="st">"review"</span>]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>test_df.columns <span class="op">=</span> [<span class="st">"rating"</span>, <span class="st">"title"</span>, <span class="st">"review"</span>]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Change the rating values to "positive" and "negative"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"rating"</span>] <span class="op">=</span> train_df[<span class="st">"rating"</span>].<span class="bu">map</span>({<span class="dv">1</span>: <span class="st">"negative"</span>, <span class="dv">2</span>: <span class="st">"positive"</span>})</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"rating"</span>] <span class="op">=</span> test_df[<span class="st">"rating"</span>].<span class="bu">map</span>({<span class="dv">1</span>: <span class="st">"negative"</span>, <span class="dv">2</span>: <span class="st">"positive"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The dataset is too huge. To save computational resources, we are going to sample 1% of the data to analyze. All the methods should still work on the full dataset though.</p>
<div id="95d23ed7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 1% of the data for each rating category</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>random_state <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> (</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    train_df.groupby(<span class="st">"rating"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span>random_state))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    .reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> (</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    test_df.groupby(<span class="st">"rating"</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span>random_state))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    .reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshuffle the sampled data</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_df.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span>random_state).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> test_df.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span>random_state).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Just want to double check that the dataset is still balanced after sampling.</p>
<div id="292bdfba" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the distribution of ratings</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>train_rating_counts <span class="op">=</span> train_df[<span class="st">"rating"</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>test_rating_counts <span class="op">=</span> test_df[<span class="st">"rating"</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for plotting</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>rating_distribution <span class="op">=</span> pd.DataFrame(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"Train"</span>: train_rating_counts, <span class="st">"Test"</span>: test_rating_counts}</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>).T</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the stacked bar plot</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>rating_distribution.plot(kind<span class="op">=</span><span class="st">"bar"</span>, stacked<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>[<span class="st">"#13bc05"</span>, <span class="st">"#ff5733"</span>])</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Distribution of Ratings (Train vs Test)"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Dataset"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Proportion"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">"Rating"</span>, labels<span class="op">=</span>[<span class="st">"Positive"</span>, <span class="st">"Negative"</span>])</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here are some example reviews from the sampled data:</p>
<div id="88bb1caa" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.head())</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a negative review</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.loc[train_df.rating <span class="op">==</span> <span class="st">"negative"</span>, <span class="st">"review"</span>].iloc[<span class="dv">200</span>][:<span class="dv">500</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a positive review</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.loc[train_df.rating <span class="op">==</span> <span class="st">"positive"</span>, <span class="st">"review"</span>].iloc[<span class="dv">200</span>][:<span class="dv">500</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Makes sense! Let’s get to the modeling.</p>
</section>
<section id="embedding-extraction" class="level2">
<h2 class="anchored" data-anchor-id="embedding-extraction">Embedding Extraction</h2>
<p>Let’s first extract features, in this case embeddings, which are internal activations of Large Language Models (LLMs). LLMs relies on the Transformer architecture to sculpt the embedding of a given word based on the surrounding context. The model is composed of a repeated circuit motif—called the “attention head” — by which the model can “attend” to surrounding words in the context window when determining the meaning of the current word.</p>
<p>Here we use a model named <a href="https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2">paraphrase-MiniLM-L3-v2</a>. This is a really small model (relative to the other current LLMs) with about 61 million parameters. It is composed of 3 layers, each of which contains each of which contains 12 attention heads that influence the embedding as it proceeds to the subsequent layer. The embeddings at each layer of the model comprise 384 features and the context window includes the surrounding 512 tokens.</p>
<p>Note that usually we get word embeddings from the LLMs. To simply things, here we directly gets a 768 dimension vector for the whole sentence or paragraph, using the <a href="https://sbert.net/index.html">sentence transformer</a> library.</p>
<p>To access this model, we have to register an account on Huggingface and get an access token. If you are curious, <a href="https://huggingface.co/docs/hub/en/security-tokens">this page</a> has more information about it. Here, we store the access token in a <code>.env</code> file under the variable <code>HUG_KEY</code> and load it through the <code>dotenv</code> package.</p>
<div id="0fa2e8f1" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"paraphrase-MiniLM-L3-v2"</span>, use_auth_token<span class="op">=</span>os.environ[<span class="st">"HUG_KEY"</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now that we have loaded the model, we are going to generate three types of embeddings. First, we use the titles of the reviews to generate embeddings, which we call <code>title_emb</code>. Second, we use the full reviews to generate embeddings, which we call <code>review_emb</code>. Note that if a review is longer than 514 words, then model will only use the first 514 words. One way to improve this is to seperate the full reviews into sentences, generate embeddings for each sentence, and then average the sentence embeddings. This we call <code>sent_emb</code>.</p>
<p>We first look at what <code>sent_tokenize</code> from the <code>nltk</code> package does:</p>
<div id="84f4c30d" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a negative review</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.loc[<span class="dv">0</span>, <span class="st">"review"</span>][:<span class="dv">500</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sentence tokenizer</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>nltk.sent_tokenize(train_df.loc[<span class="dv">0</span>, <span class="st">"review"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we will define the functions for embedding extractions and extract the three types of embeddings.</p>
<div id="27229d68" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> full_text_emb(model, text):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> model.encode(text, show_progress_bar<span class="op">=</span><span class="va">True</span>)  <span class="co"># encode the full text</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sent_text_emb(model, text):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    sent_text <span class="op">=</span> nltk.sent_tokenize(text)  <span class="co"># tokenize the text into sentences</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> model.encode(sent_text, show_progress_bar<span class="op">=</span><span class="va">True</span>)  <span class="co"># batch encode</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> np.mean(emb, axis<span class="op">=</span><span class="dv">0</span>).shape  <span class="co"># mean of the sentence embeddings</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract embeddings</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"title_emb"</span>] <span class="op">=</span> train_df[<span class="st">"title"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"review_emb"</span>] <span class="op">=</span> train_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"sent_emb"</span>] <span class="op">=</span> train_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sent_text_emb(model, x))</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"title_emb"</span>] <span class="op">=</span> test_df[<span class="st">"title"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"review_emb"</span>] <span class="op">=</span> test_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"sent_emb"</span>] <span class="op">=</span> test_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sent_text_emb(model, x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Because we have so much data, generating embeddings still takes a lot of computational resources even after the 1% sampling. Thus, we are just going to load the already generated <code>title_emb</code> and <code>review_emb</code> and ignore <code>sent_emb</code> for now. If you are interested, here is the <a href="https://github.com/VeritasJoker/PSY-504-Quarto/blob/main/posts/05-01%20Final%20Project/emb_extract.py">script</a> we used to batch the input and speed up the embedding extraction process.</p>
<div id="711e062e" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the sampled csv</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">"train_s.csv"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">"test_s.csv"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the generated embeddings</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>train_review_emb <span class="op">=</span> np.load(<span class="st">"train_review_embs.npy"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>train_title_emb <span class="op">=</span> np.load(<span class="st">"train_title_embs.npy"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>test_review_emb <span class="op">=</span> np.load(<span class="st">"test_review_embs.npy"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>test_title_emb <span class="op">=</span> np.load(<span class="st">"test_title_embs.npy"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the y column as array</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> np.array(train_df.rating)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> np.array(test_df.rating)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_review_emb.shape, train_title_emb.shape, train_y.shape)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_review_emb.shape, test_title_emb.shape, test_y.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="classification" class="level2">
<h2 class="anchored" data-anchor-id="classification">Classification</h2>
<p>Now we have both the embeddings and the corresponding review results (positive and negative), we can start to perform classification. We will use a few classifiers, including Logistic Regression and Ridge Regression Classifier. We will also employ some ensemble methods, like Adaboost and Random Forest. We will first define each of them in separate functions.</p>
<p>First we will define a scale function that scales all input data.</p>
<div id="3fe60000" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Scale(data):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scales data</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> preprocessing.StandardScaler().fit(data)  <span class="co"># scaler</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> scaler.transform(data)  <span class="co"># scale training data</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here are the classifiers, including logistic regression without penalty, logistic regression with elastic-net penalty, and ridge classifier. We customize the number of iteraions through the <code>max_iter</code> parameter. If you are curious about each methods and the function that is implemented in Python, here are more info on sklearn about <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">logistic regression</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html">ridge classifier</a>.</p>
<div id="65074701" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LogReg(train_x, train_y, test_x, test_y, <span class="bu">iter</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logistic</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> Scale(train_x)  <span class="co"># scale training data</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> Scale(test_x)  <span class="co"># scale training data</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    logreg <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="va">None</span>, solver<span class="op">=</span><span class="st">"saga"</span>, max_iter<span class="op">=</span><span class="bu">iter</span>).fit(</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        train_x, train_y</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logreg.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ElasticNet(train_x, train_y, test_x, test_y, <span class="bu">iter</span>):</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logistic elastic net</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> Scale(train_x)  <span class="co"># scale training data</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> Scale(test_x)  <span class="co"># scale training data</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    logreg <span class="op">=</span> LogisticRegression(</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        penalty<span class="op">=</span><span class="st">"elasticnet"</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, solver<span class="op">=</span><span class="st">"saga"</span>, max_iter<span class="op">=</span><span class="bu">iter</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    ).fit(train_x, train_y)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logreg.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Ridge(train_x, train_y, test_x, test_y, <span class="bu">iter</span>):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ridge</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    ridge <span class="op">=</span> RidgeClassifier(max_iter<span class="op">=</span><span class="bu">iter</span>).fit(train_x, train_y)  <span class="co"># ridge</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ridge.score(test_x, test_y)  <span class="co"># return accuracy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here are the ensemble methods, including <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">Adaboost</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random Forest</a>. Here, we customize the number of parameters for Adaboost and the number of trees for Random Forest through the <code>n_estimators</code> parameter.</p>
<div id="64719813" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> AdaBoost(train_x, train_y, test_x, test_y, n_est):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adaboost</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    ada <span class="op">=</span> AdaBoostClassifier(n_estimators<span class="op">=</span>n_est).fit(train_x, train_y)  <span class="co"># adaboost</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ada.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RandForest(train_x, train_y, test_x, test_y, n_est):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># random forest</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    forest <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span>n_est).fit(</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        train_x, train_y</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># build forest</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> forest.score(test_x, test_y)  <span class="co"># return accuracy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Some of the classification methods, especially Adaboost, takes a long time to run. So we are not going to actually run them here. Here are the some sample code to run logistic regression for <code>review_emb</code> and <code>title_emb</code>. The actual script we used is <a href="https://github.com/VeritasJoker/PSY-504-Quarto/blob/main/posts/05-01%20Final%20Project/classifier.py">here</a>.</p>
<div id="86383abe" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>review_log_score <span class="op">=</span> LogReg(train_review_emb, train_y, test_reiew_emb, test_y, <span class="dv">500</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>title_log_score <span class="op">=</span> LogReg(train_title_emb, train_y, test_title_emb, test_y, <span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Instead, we will directly load a summary csv of all the classification results and checkout how it looks.</p>
<div id="5a072e60" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>classification_results <span class="op">=</span> pd.read_csv(<span class="st">"scores.csv"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>classification_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can also plot the scores on a bar plot.</p>
<div id="0116db92" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>sns.catplot(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>classification_results,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"bar"</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Embedding Type"</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"Classification Score"</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"Classification Model"</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    errorbar<span class="op">=</span><span class="st">"sd"</span>,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    palette<span class="op">=</span><span class="st">"dark"</span>,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>For all our methods, the classification accuracy is about 0.8. It seems that there isn’t much difference between our two types of embeddings and the classification methods.</p>
<p>The ensemble methods, Adaboost and Random Forest, perform a bit worse. They are in general more complicated and potentially require more finetuning with the parameters, which we did not do in this tutorial.</p>
<p>Using the titles for embeddings is on par with using the full review, which is pretty surprising but makes sense. There probably is enough info in the review title most of the times to determine if the review is positive or negative.</p>
<p>Again, this is just a simple tutorial demonstrating some classification methods. In order to perform a comprehensive classification study, here are some more steps to include: - Using better models for embedding. The current model is super small in terms of LLMs and previous research have shown that bigger and better trained models will have better performance.</p>
<ul>
<li><p>Implementing the third type of embedding <code>sent_emb</code>, which will incorporate the full review into the embedding.</p></li>
<li><p>Perform some parameter search with our models, especially with the ensemble methods.</p></li>
<li><p>Use the full data. Here we are only sampling 1% of the data.</p></li>
</ul>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Final Project"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Princeton University"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-05-03"</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "KW"</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [code, analysis]</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    self-contained: false</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    anchor-sections: true</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-width: 8</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">    fig-height: 4</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-bg: "#f1f3f5"</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">    code-block-border-left: "#31BAE9"</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">    mainfont: Source Sans Pro</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">    theme: journal</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-location: left</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co">    captions: true</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co">    cap-location: margin</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co">    table-captions: true</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co">    tbl-cap-location: margin</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co">    reference-location: margin</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf:</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co">    pdf-engine: lualatex</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: false</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co">    number-sections: true</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co">    number-depth: 2</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co">    top-level-division: section</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="co">    reference-location: document</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="co">    listings: false</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co">    header-includes:</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co">      \usepackage{marginnote, here, relsize, needspace, setspace}</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="co">      \def\it{\emph}</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="co">  freeze: auto</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="co">  message: false</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="co">  fig-align: center</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="co">  fig-width: 12</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="co">  fig-height: 8</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="co">  editor_options: </span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a><span class="co">  chunk_output_type: inline</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a><span class="co">  code-overflow: wrap</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="an">editor:</span><span class="co"> visual</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>This is a tutorial to perform binary classifications for the <span class="co">[</span><span class="ot">Amazon Reviews</span><span class="co">](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews/data)</span> dataset.</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>The Amazon reviews dataset consists of reviews from Amazon. The data span a period of 18 years, including \~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. Here, we take review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 are ignored.</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loading Packages &amp; Dataset</span></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{python, warning=FALSE}</span></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="co"># os</span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a><span class="co"># nlp</span></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a><span class="co"># classification</span></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier, RandomForestClassifier</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting</span></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.colors <span class="im">as</span> mcolors</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">"train.csv"</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">"test.csv"</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## EDA</span></span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Length of the data</span></span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(train_df))</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(test_df))</span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter out NaNs</span></span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a>train_df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a>test_df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Name the columns</span></span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a>train_df.columns <span class="op">=</span> [<span class="st">"rating"</span>, <span class="st">"title"</span>, <span class="st">"review"</span>]</span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a>test_df.columns <span class="op">=</span> [<span class="st">"rating"</span>, <span class="st">"title"</span>, <span class="st">"review"</span>]</span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a><span class="co"># Change the rating values to "positive" and "negative"</span></span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"rating"</span>] <span class="op">=</span> train_df[<span class="st">"rating"</span>].<span class="bu">map</span>({<span class="dv">1</span>: <span class="st">"negative"</span>, <span class="dv">2</span>: <span class="st">"positive"</span>})</span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"rating"</span>] <span class="op">=</span> test_df[<span class="st">"rating"</span>].<span class="bu">map</span>({<span class="dv">1</span>: <span class="st">"negative"</span>, <span class="dv">2</span>: <span class="st">"positive"</span>})</span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a>The dataset is too huge. To save computational resources, we are going to sample 1% of the data to analyze. All the methods should still work on the full dataset though.</span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a><span class="co"># | warning: false</span></span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 1% of the data for each rating category</span></span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a>random_state <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> (</span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a>    train_df.groupby(<span class="st">"rating"</span>)</span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span>random_state))</span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a>    .reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> (</span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a>    test_df.groupby(<span class="st">"rating"</span>)</span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.sample(frac<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span>random_state))</span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a>    .reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshuffle the sampled data</span></span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_df.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span>random_state).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> test_df.sample(frac<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span>random_state).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a>Just want to double check that the dataset is still balanced after sampling.</span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the distribution of ratings</span></span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a>train_rating_counts <span class="op">=</span> train_df[<span class="st">"rating"</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a>test_rating_counts <span class="op">=</span> test_df[<span class="st">"rating"</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for plotting</span></span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a>rating_distribution <span class="op">=</span> pd.DataFrame(</span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"Train"</span>: train_rating_counts, <span class="st">"Test"</span>: test_rating_counts}</span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a>).T</span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the stacked bar plot</span></span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a>rating_distribution.plot(kind<span class="op">=</span><span class="st">"bar"</span>, stacked<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>[<span class="st">"#13bc05"</span>, <span class="st">"#ff5733"</span>])</span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Distribution of Ratings (Train vs Test)"</span>)</span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Dataset"</span>)</span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Proportion"</span>)</span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">"Rating"</span>, labels<span class="op">=</span>[<span class="st">"Positive"</span>, <span class="st">"Negative"</span>])</span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a>Here are some example reviews from the sampled data:</span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.head())</span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a negative review</span></span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.loc[train_df.rating <span class="op">==</span> <span class="st">"negative"</span>, <span class="st">"review"</span>].iloc[<span class="dv">200</span>][:<span class="dv">500</span>])</span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a positive review</span></span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.loc[train_df.rating <span class="op">==</span> <span class="st">"positive"</span>, <span class="st">"review"</span>].iloc[<span class="dv">200</span>][:<span class="dv">500</span>])</span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a>Makes sense! Let's get to the modeling.</span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a><span class="fu">## Embedding Extraction</span></span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a>Let's first extract features, in this case embeddings, which are internal activations of Large Language Models (LLMs). LLMs relies on the Transformer architecture to sculpt the embedding of a given word based on the surrounding context. The model is composed of a repeated circuit motif—called the "attention head" — by which the model can "attend" to surrounding words in the context window when determining the meaning of the current word.</span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a>Here we use a model named <span class="co">[</span><span class="ot">paraphrase-MiniLM-L3-v2</span><span class="co">](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2)</span>. This is a really small model (relative to the other current LLMs) with about 61 million parameters. It is composed of 3 layers, each of which contains each of which contains 12 attention heads that influence the embedding as it proceeds to the subsequent layer. The embeddings at each layer of the model comprise 384 features and the context window includes the surrounding 512 tokens.</span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a>Note that usually we get word embeddings from the LLMs. To simply things, here we directly gets a 768 dimension vector for the whole sentence or paragraph, using the <span class="co">[</span><span class="ot">sentence transformer</span><span class="co">](https://sbert.net/index.html)</span> library.</span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a>To access this model, we have to register an account on Huggingface and get an access token. If you are curious, <span class="co">[</span><span class="ot">this page</span><span class="co">](https://huggingface.co/docs/hub/en/security-tokens)</span> has more information about it. Here, we store the access token in a <span class="in">`.env`</span> file under the variable <span class="in">`HUG_KEY`</span> and load it through the <span class="in">`dotenv`</span> package.</span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a><span class="in">```{python, eval=FALSE}</span></span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(</span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a>    <span class="st">"paraphrase-MiniLM-L3-v2"</span>, use_auth_token<span class="op">=</span>os.environ[<span class="st">"HUG_KEY"</span>]</span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a>Now that we have loaded the model, we are going to generate three types of embeddings. First, we use the titles of the reviews to generate embeddings, which we call <span class="in">`title_emb`</span>. Second, we use the full reviews to generate embeddings, which we call <span class="in">`review_emb`</span>. Note that if a review is longer than 514 words, then model will only use the first 514 words. One way to improve this is to seperate the full reviews into sentences, generate embeddings for each sentence, and then average the sentence embeddings. This we call <span class="in">`sent_emb`</span>.</span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a>We first look at what <span class="in">`sent_tokenize`</span> from the <span class="in">`nltk`</span> package does:</span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a negative review</span></span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.loc[<span class="dv">0</span>, <span class="st">"review"</span>][:<span class="dv">500</span>])</span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a><span class="co"># Sentence tokenizer</span></span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a>nltk.sent_tokenize(train_df.loc[<span class="dv">0</span>, <span class="st">"review"</span>])</span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a>Now we will define the functions for embedding extractions and extract the three types of embeddings.</span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> full_text_emb(model, text):</span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> model.encode(text, show_progress_bar<span class="op">=</span><span class="va">True</span>)  <span class="co"># encode the full text</span></span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb</span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sent_text_emb(model, text):</span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a>    sent_text <span class="op">=</span> nltk.sent_tokenize(text)  <span class="co"># tokenize the text into sentences</span></span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> model.encode(sent_text, show_progress_bar<span class="op">=</span><span class="va">True</span>)  <span class="co"># batch encode</span></span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> np.mean(emb, axis<span class="op">=</span><span class="dv">0</span>).shape  <span class="co"># mean of the sentence embeddings</span></span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb</span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract embeddings</span></span>
<span id="cb17-236"><a href="#cb17-236" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"title_emb"</span>] <span class="op">=</span> train_df[<span class="st">"title"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb17-237"><a href="#cb17-237" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"review_emb"</span>] <span class="op">=</span> train_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a>train_df[<span class="st">"sent_emb"</span>] <span class="op">=</span> train_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sent_text_emb(model, x))</span>
<span id="cb17-239"><a href="#cb17-239" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"title_emb"</span>] <span class="op">=</span> test_df[<span class="st">"title"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb17-240"><a href="#cb17-240" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"review_emb"</span>] <span class="op">=</span> test_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: full_text_emb(model, x))</span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">"sent_emb"</span>] <span class="op">=</span> test_df[<span class="st">"review"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: sent_text_emb(model, x))</span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a>Because we have so much data, generating embeddings still takes a lot of computational resources even after the 1% sampling. Thus, we are just going to load the already generated <span class="in">`title_emb`</span> and <span class="in">`review_emb`</span> and ignore <span class="in">`sent_emb`</span> for now. If you are interested, here is the <span class="co">[</span><span class="ot">script</span><span class="co">](https://github.com/VeritasJoker/PSY-504-Quarto/blob/main/posts/05-01%20Final%20Project/emb_extract.py)</span> we used to batch the input and speed up the embedding extraction process.</span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the sampled csv</span></span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">"train_s.csv"</span>)</span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">"test_s.csv"</span>)</span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the generated embeddings</span></span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a>train_review_emb <span class="op">=</span> np.load(<span class="st">"train_review_embs.npy"</span>)</span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a>train_title_emb <span class="op">=</span> np.load(<span class="st">"train_title_embs.npy"</span>)</span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a>test_review_emb <span class="op">=</span> np.load(<span class="st">"test_review_embs.npy"</span>)</span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a>test_title_emb <span class="op">=</span> np.load(<span class="st">"test_title_embs.npy"</span>)</span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-259"><a href="#cb17-259" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the y column as array</span></span>
<span id="cb17-260"><a href="#cb17-260" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> np.array(train_df.rating)</span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> np.array(test_df.rating)</span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_review_emb.shape, train_title_emb.shape, train_y.shape)</span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_review_emb.shape, test_title_emb.shape, test_y.shape)</span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classification</span></span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-269"><a href="#cb17-269" aria-hidden="true" tabindex="-1"></a>Now we have both the embeddings and the corresponding review results (positive and negative), we can start to perform classification. We will use a few classifiers, including Logistic Regression and Ridge Regression Classifier. We will also employ some ensemble methods, like Adaboost and Random Forest. We will first define each of them in separate functions.</span>
<span id="cb17-270"><a href="#cb17-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-271"><a href="#cb17-271" aria-hidden="true" tabindex="-1"></a>First we will define a scale function that scales all input data.</span>
<span id="cb17-272"><a href="#cb17-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-275"><a href="#cb17-275" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-276"><a href="#cb17-276" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Scale(data):</span>
<span id="cb17-277"><a href="#cb17-277" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scales data</span></span>
<span id="cb17-278"><a href="#cb17-278" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> preprocessing.StandardScaler().fit(data)  <span class="co"># scaler</span></span>
<span id="cb17-279"><a href="#cb17-279" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> scaler.transform(data)  <span class="co"># scale training data</span></span>
<span id="cb17-280"><a href="#cb17-280" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span>
<span id="cb17-281"><a href="#cb17-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-282"><a href="#cb17-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-283"><a href="#cb17-283" aria-hidden="true" tabindex="-1"></a>Here are the classifiers, including logistic regression without penalty, logistic regression with elastic-net penalty, and ridge classifier. We customize the number of iteraions through the <span class="in">`max_iter`</span> parameter. If you are curious about each methods and the function that is implemented in Python, here are more info on sklearn about <span class="co">[</span><span class="ot">logistic regression</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)</span> and <span class="co">[</span><span class="ot">ridge classifier</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)</span>.</span>
<span id="cb17-284"><a href="#cb17-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-287"><a href="#cb17-287" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-288"><a href="#cb17-288" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LogReg(train_x, train_y, test_x, test_y, <span class="bu">iter</span>):</span>
<span id="cb17-289"><a href="#cb17-289" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logistic</span></span>
<span id="cb17-290"><a href="#cb17-290" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> Scale(train_x)  <span class="co"># scale training data</span></span>
<span id="cb17-291"><a href="#cb17-291" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> Scale(test_x)  <span class="co"># scale training data</span></span>
<span id="cb17-292"><a href="#cb17-292" aria-hidden="true" tabindex="-1"></a>    logreg <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="va">None</span>, solver<span class="op">=</span><span class="st">"saga"</span>, max_iter<span class="op">=</span><span class="bu">iter</span>).fit(</span>
<span id="cb17-293"><a href="#cb17-293" aria-hidden="true" tabindex="-1"></a>        train_x, train_y</span>
<span id="cb17-294"><a href="#cb17-294" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-295"><a href="#cb17-295" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logreg.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb17-296"><a href="#cb17-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-297"><a href="#cb17-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-298"><a href="#cb17-298" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ElasticNet(train_x, train_y, test_x, test_y, <span class="bu">iter</span>):</span>
<span id="cb17-299"><a href="#cb17-299" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logistic elastic net</span></span>
<span id="cb17-300"><a href="#cb17-300" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> Scale(train_x)  <span class="co"># scale training data</span></span>
<span id="cb17-301"><a href="#cb17-301" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> Scale(test_x)  <span class="co"># scale training data</span></span>
<span id="cb17-302"><a href="#cb17-302" aria-hidden="true" tabindex="-1"></a>    logreg <span class="op">=</span> LogisticRegression(</span>
<span id="cb17-303"><a href="#cb17-303" aria-hidden="true" tabindex="-1"></a>        penalty<span class="op">=</span><span class="st">"elasticnet"</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, solver<span class="op">=</span><span class="st">"saga"</span>, max_iter<span class="op">=</span><span class="bu">iter</span></span>
<span id="cb17-304"><a href="#cb17-304" aria-hidden="true" tabindex="-1"></a>    ).fit(train_x, train_y)</span>
<span id="cb17-305"><a href="#cb17-305" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logreg.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb17-306"><a href="#cb17-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-307"><a href="#cb17-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-308"><a href="#cb17-308" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Ridge(train_x, train_y, test_x, test_y, <span class="bu">iter</span>):</span>
<span id="cb17-309"><a href="#cb17-309" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ridge</span></span>
<span id="cb17-310"><a href="#cb17-310" aria-hidden="true" tabindex="-1"></a>    ridge <span class="op">=</span> RidgeClassifier(max_iter<span class="op">=</span><span class="bu">iter</span>).fit(train_x, train_y)  <span class="co"># ridge</span></span>
<span id="cb17-311"><a href="#cb17-311" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ridge.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb17-312"><a href="#cb17-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-313"><a href="#cb17-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-314"><a href="#cb17-314" aria-hidden="true" tabindex="-1"></a>Here are the ensemble methods, including <span class="co">[</span><span class="ot">Adaboost</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)</span> and <span class="co">[</span><span class="ot">Random Forest</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)</span>. Here, we customize the number of parameters for Adaboost and the number of trees for Random Forest through the <span class="in">`n_estimators`</span> parameter.</span>
<span id="cb17-315"><a href="#cb17-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-318"><a href="#cb17-318" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-319"><a href="#cb17-319" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> AdaBoost(train_x, train_y, test_x, test_y, n_est):</span>
<span id="cb17-320"><a href="#cb17-320" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adaboost</span></span>
<span id="cb17-321"><a href="#cb17-321" aria-hidden="true" tabindex="-1"></a>    ada <span class="op">=</span> AdaBoostClassifier(n_estimators<span class="op">=</span>n_est).fit(train_x, train_y)  <span class="co"># adaboost</span></span>
<span id="cb17-322"><a href="#cb17-322" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ada.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb17-323"><a href="#cb17-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-324"><a href="#cb17-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-325"><a href="#cb17-325" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RandForest(train_x, train_y, test_x, test_y, n_est):</span>
<span id="cb17-326"><a href="#cb17-326" aria-hidden="true" tabindex="-1"></a>    <span class="co"># random forest</span></span>
<span id="cb17-327"><a href="#cb17-327" aria-hidden="true" tabindex="-1"></a>    forest <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span>n_est).fit(</span>
<span id="cb17-328"><a href="#cb17-328" aria-hidden="true" tabindex="-1"></a>        train_x, train_y</span>
<span id="cb17-329"><a href="#cb17-329" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># build forest</span></span>
<span id="cb17-330"><a href="#cb17-330" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> forest.score(test_x, test_y)  <span class="co"># return accuracy</span></span>
<span id="cb17-331"><a href="#cb17-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-332"><a href="#cb17-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-333"><a href="#cb17-333" aria-hidden="true" tabindex="-1"></a>Some of the classification methods, especially Adaboost, takes a long time to run. So we are not going to actually run them here. Here are the some sample code to run logistic regression for <span class="in">`review_emb`</span> and <span class="in">`title_emb`</span>. The actual script we used is <span class="co">[</span><span class="ot">here</span><span class="co">](https://github.com/VeritasJoker/PSY-504-Quarto/blob/main/posts/05-01%20Final%20Project/classifier.py)</span>.</span>
<span id="cb17-334"><a href="#cb17-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-337"><a href="#cb17-337" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-338"><a href="#cb17-338" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb17-339"><a href="#cb17-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-340"><a href="#cb17-340" aria-hidden="true" tabindex="-1"></a>review_log_score <span class="op">=</span> LogReg(train_review_emb, train_y, test_reiew_emb, test_y, <span class="dv">500</span>)</span>
<span id="cb17-341"><a href="#cb17-341" aria-hidden="true" tabindex="-1"></a>title_log_score <span class="op">=</span> LogReg(train_title_emb, train_y, test_title_emb, test_y, <span class="dv">500</span>)</span>
<span id="cb17-342"><a href="#cb17-342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-343"><a href="#cb17-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-344"><a href="#cb17-344" aria-hidden="true" tabindex="-1"></a>Instead, we will directly load a summary csv of all the classification results and checkout how it looks.</span>
<span id="cb17-345"><a href="#cb17-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-348"><a href="#cb17-348" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-349"><a href="#cb17-349" aria-hidden="true" tabindex="-1"></a>classification_results <span class="op">=</span> pd.read_csv(<span class="st">"scores.csv"</span>)</span>
<span id="cb17-350"><a href="#cb17-350" aria-hidden="true" tabindex="-1"></a>classification_results</span>
<span id="cb17-351"><a href="#cb17-351" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-352"><a href="#cb17-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-353"><a href="#cb17-353" aria-hidden="true" tabindex="-1"></a>We can also plot the scores on a bar plot.</span>
<span id="cb17-354"><a href="#cb17-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-357"><a href="#cb17-357" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-358"><a href="#cb17-358" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb17-359"><a href="#cb17-359" aria-hidden="true" tabindex="-1"></a>sns.catplot(</span>
<span id="cb17-360"><a href="#cb17-360" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>classification_results,</span>
<span id="cb17-361"><a href="#cb17-361" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"bar"</span>,</span>
<span id="cb17-362"><a href="#cb17-362" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Embedding Type"</span>,</span>
<span id="cb17-363"><a href="#cb17-363" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"Classification Score"</span>,</span>
<span id="cb17-364"><a href="#cb17-364" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"Classification Model"</span>,</span>
<span id="cb17-365"><a href="#cb17-365" aria-hidden="true" tabindex="-1"></a>    errorbar<span class="op">=</span><span class="st">"sd"</span>,</span>
<span id="cb17-366"><a href="#cb17-366" aria-hidden="true" tabindex="-1"></a>    palette<span class="op">=</span><span class="st">"dark"</span>,</span>
<span id="cb17-367"><a href="#cb17-367" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb17-368"><a href="#cb17-368" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb17-369"><a href="#cb17-369" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-370"><a href="#cb17-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-371"><a href="#cb17-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-372"><a href="#cb17-372" aria-hidden="true" tabindex="-1"></a><span class="fu">## Results</span></span>
<span id="cb17-373"><a href="#cb17-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-374"><a href="#cb17-374" aria-hidden="true" tabindex="-1"></a>For all our methods, the classification accuracy is about 0.8. It seems that there isn't much difference between our two types of embeddings and the classification methods.</span>
<span id="cb17-375"><a href="#cb17-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-376"><a href="#cb17-376" aria-hidden="true" tabindex="-1"></a>The ensemble methods, Adaboost and Random Forest, perform a bit worse. They are in general more complicated and potentially require more finetuning with the parameters, which we did not do in this tutorial.</span>
<span id="cb17-377"><a href="#cb17-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-378"><a href="#cb17-378" aria-hidden="true" tabindex="-1"></a>Using the titles for embeddings is on par with using the full review, which is pretty surprising but makes sense. There probably is enough info in the review title most of the times to determine if the review is positive or negative.</span>
<span id="cb17-379"><a href="#cb17-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-380"><a href="#cb17-380" aria-hidden="true" tabindex="-1"></a>Again, this is just a simple tutorial demonstrating some classification methods. In order to perform a comprehensive classification study, here are some more steps to include: - Using better models for embedding. The current model is super small in terms of LLMs and previous research have shown that bigger and better trained models will have better performance.</span>
<span id="cb17-381"><a href="#cb17-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-382"><a href="#cb17-382" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Implementing the third type of embedding <span class="in">`sent_emb`</span>, which will incorporate the full review into the embedding.</span>
<span id="cb17-383"><a href="#cb17-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-384"><a href="#cb17-384" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Perform some parameter search with our models, especially with the ensemble methods.</span>
<span id="cb17-385"><a href="#cb17-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-386"><a href="#cb17-386" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Use the full data. Here we are only sampling 1% of the data.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>