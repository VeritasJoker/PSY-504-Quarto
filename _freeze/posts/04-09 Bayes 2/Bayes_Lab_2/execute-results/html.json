{
  "hash": "59d29ed614b5821174f552cf02348c4a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes_Lab_2\"\nsubtitle: \"Princeton University\"\ndate: \"2025-04-09\"\nauthor: \"KW\"\ncategories: [code, analysis]\nformat:\n  html:\n    self-contained: false\neditor: visual\n---\n\n\nFor Lab 1, you had explored the data and looked at models built via lm() and via brms(using default priors). You had also drawn posterior samples after fitting the model.\n\nFor Lab 2, we continue with the Palmer Penguins. And we will look more at distributions and priors.\n\nAgain, there will be conceptual questions to answer as you work through this example, and exercises.\n\n# Part 3: Distributions all the way down\n\nGiven it's a continuation of Lab 1, let's begin by loading relevant packages, cleaning/pre-processing the data, and fitting lm() and the default brm models\n\n## Setup: Packages and data\n\nWe load the primary packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\n```\n:::\n\n\nWe want the same data set up as in the last lab.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the penguins data\ndata(penguins, package = \"palmerpenguins\")\n\n# subset the data\nchinstrap <- penguins %>% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 68\nColumns: 8\n$ species           <fct> Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            <fct> Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    <dbl> 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     <dbl> 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm <int> 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       <int> 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               <fct> female, male, male, female, male, female, female, ma…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n:::\n\n\n## Models\n\nOnce again, we'll fit the model\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) ,\n\\end{align}\n$$\n\nwith both `lm()` and `brm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS\nfit1.ols <- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# Bayes\nfit1.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\n## Bayesians have many kinds of distributions\n\nIn Bayesian statistics, we have at least 6 distributions to keep track of. Those are:\n\n-   the likelihood distributions\n-   the prior parameter distribution (aka priors)\n-   the prior predictive distributions\n-   the posterior parameter distributions (aka posteriors)\n-   the posterior-predictive distribution\n\nIn many respect, it's distributions 'all the way down,' with Bayesians. This can be indeed be difficult to keep track of at first. But since this is true for any class of Bayesian models (not just regression), you'll hopefully get used to it.\\\n\n### QUESTION 1: How would you represent these 6 distributions mathematically, using $P_0$'$P$, $D$, $|$, and $\\theta$ ?\n\n::: callout-tip\nHint 1: Many of these terms were in the Bayes Rule.\n:::\n\n### Answer:\n\nLikelihood: $P(D|\\theta)$\n\nPrior parameter distribution: $P_0(\\theta)$\n\nPrior predictive distribution $P0_0(D)=\\int P(D∣\\theta)P_0​(\\theta)d\\theta$\n\nPosterior parameter distribution: $P(\\theta∣D)$\n\nPosterior predictive distribution: $P(D_n​∣D)=\\int P(D_n​∣\\theta)P(\\theta∣D)d\\theta$\n\nWe also have some other distributions that follow from these. For example, - the distributions of the model expectations (i.e., the predicted means)\n\n### Likelihood distributions.\n\nWe are approaching Bayesian statistics from a likelihood-based perspective. That is, we situate regression models within the greater context of a likelihood function. (There are ways to do non-parametric Bayesian statistics, which don't focus on likelihoods. We won't get into that right now.)\n\nSo far, we have been using the conventional Gaussian likelihood. If we have some variable $y$, we can express it as normally distributed by\n\n$$\n\\operatorname{Normal}(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{1}{2} \\left( \\frac{y - \\mu}{\\sigma}\\right)^2\\right),\n$$\n\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation. With this likelihood,\n\n-   $\\mu \\in \\mathbb R$\n    -   the mean can be any real number, ranging from $-\\infty$ to $\\infty$\n-   $\\sigma \\in \\mathbb R_{> 0}$\n    -   the standard deviation can take on any real number greater than zero.\n\nIt's also the assumption\n\n-   $y \\in \\mathbb R$\n    -   the focal variable $y$ can be any real number, ranging from $-\\infty$ to $\\infty$.\n\nOne of the ways we wrote our model formula back in the first file was\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n$$\n\nand further in the discussion, we updated that equation with the posterior means for our three parameters to\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, 2.92) \\\\\n\\mu_i & = 32.2 + 0.004 \\text{body_mass_g}_i.\n\\end{align}\n$$\n\nBefore we get into this, though, let's back up and consider an intercept-only model of the form\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 ,\n\\end{align}\n$$\n\nwhere there is no predictor variable. Here's how to fit the model with `brm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bayes\nfit0.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n\nLet's look at the model summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit0.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    48.85      0.39    48.09    49.61 1.00     3752     3087\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.38      0.29     2.87     4.01 1.00     2869     2223\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nThe intercept parameter $\\beta_0$ is a stand-in for $\\mu$. The $\\sigma$ parameter is just $\\sigma$. Here they are in a plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- as_draws_df(fit0.b) \n\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nHere are the posterior means for those two parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- mean(draws$b_Intercept)\nsigma <- mean(draws$sigma)\n\nmu; sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 48.84845\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.381167\n```\n\n\n:::\n:::\n\n\nWe can use `dnorm()` to compute the shape of $\\operatorname{Normal}(48.8, 3.4)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %>% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %>% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can compare this to the sample distribution of the `bill_length_mm` data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchinstrap %>% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\")\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIt's not a great fit, but not horrible either.\n\nNow let's see what this means for our univariable model `fit1.b`. First, let's learn about the `posterior_summary()` function, which we'll use to save a few posterior means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.215255e+01 3.4746652759  2.524819e+01   39.07563171\nb_body_mass_g  4.472171e-03 0.0009249245  2.639604e-03    0.00633873\nsigma          2.927355e+00 0.2539166609  2.486633e+00    3.48115874\nIntercept      4.884756e+01 0.3637706059  4.813924e+01   49.54907802\nlprior        -4.298903e+00 0.0697319313 -4.457295e+00   -4.18521472\nlp__          -1.723167e+02 1.2311421580 -1.754332e+02 -170.89440663\n```\n\n\n:::\n\n```{.r .cell-code}\nb0    <- posterior_summary(fit1.b)[1, 1]\nb1    <- posterior_summary(fit1.b)[2, 1]\nsigma <- posterior_summary(fit1.b)[3, 1]\n```\n:::\n\n\nNow we plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %>% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nOur univariable model `fit1.b` can be viewed as something like a 3-dimensional Gaussian hill.\n\n### Prior distributions & Prior predictive distributions.\n\nLet's hold off on this for a bit.\n\n### Parameter distributions.\n\nUp above, we plotted the posterior distributions for our intercept-only `fit0.b` model. Here they are again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws %>% \n  rename(`beta[0]==mu` = b_Intercept) %>% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWe might practice making a similar plot for our univariable model `fit1.b`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(fit1.b) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nSome authors, like John Kruschke, have a strong preference for plotting their posteriors with histograms, rather than density plots.\n\n## Distributions of the model expectations.\n\nTake another look at the `conditional_effects()` plot from earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit1.b) %>% \n  plot(points = TRUE)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe blue line is the posterior mean, for the $\\mu_i$, the model-based mean for `bill_length_mm`, given the value for the predictor `body_mass_g`. The semitransparent gray ribbon marks the percentile-based interval for the conditional mean.\n\nWe can make a similar plot with the `fitted()` function. First we'll need a predictor grid, we'll call `nd`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- tibble(body_mass_g = seq(\n  from = min(chinstrap$body_mass_g),\n  to = max(chinstrap$body_mass_g),\n  length.out = 100))\n\nglimpse(nd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 1\n$ body_mass_g <dbl> 2700.000, 2721.212, 2742.424, 2763.636, 2784.848, 2806.061…\n```\n\n\n:::\n:::\n\n\nNow pump `nd` into the `fitted()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.22741 1.0249946 42.21126 46.25158\n[2,] 44.32228 1.0066762 42.34378 46.31956\n[3,] 44.41714 0.9884077 42.47670 46.37415\n[4,] 44.51201 0.9701919 42.60366 46.42874\n[5,] 44.60687 0.9520320 42.73211 46.48333\n[6,] 44.70173 0.9339311 42.86239 46.54871\n```\n\n\n:::\n:::\n\n\nNow plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nLook what happens if we augment the `probs` argument in `fitted()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nNow look what happens if we set `summary = FALSE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:100] 44.3 45.2 45.1 43.1 44.5 ...\n```\n\n\n:::\n:::\n\n\nWe get full 4,000 draw posterior distributions for each of the 100 levels of the predictor `body_mass_g`. Now look at what happens if we wrangle that output a little, and plot with aid from `stat_lineribbon()` from the **ggdist** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  stat_lineribbon() +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nLook what happens when we request more intervals in the `.width` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nThe conditional mean, $\\mu_i$, has its own distribution. We can take this visualization approach even further to make a color gradient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nFor technical details on this visualization approach, go here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-gradients>.\n\nThe **ggdist** package even has an experimental visualization approach that's based on density gradients, rather than interval-width gradients. Since this is experimental, I'm not going to go into the details. But if you're curious and adventurous, you can learn more here: <https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-density-gradients>.\n\n### Posterior-predictive distributions.\n\nThe last section showed the posterior distributions for the model expectations (i.e., the conditional means). In the context of the Gaussian distribution, that's $\\mu$, or $\\mu_i$ in the case of the univariable model `fit1.b`. But the whole Gaussian distribution includes $\\mu$ and $\\sigma$.\n\nThis is where the `predict()` function comes in. First, we compare the `fitted()` output to `predict()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.22741 1.0249946 42.21126 46.25158\n[2,] 44.32228 1.0066762 42.34378 46.31956\n[3,] 44.41714 0.9884077 42.47670 46.37415\n[4,] 44.51201 0.9701919 42.60366 46.42874\n[5,] 44.60687 0.9520320 42.73211 46.48333\n[6,] 44.70173 0.9339311 42.86239 46.54871\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  # subset the first 6 rows\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.20230  3.111086 38.06235 50.43775\n[2,] 44.39949  3.108570 38.32470 50.59507\n[3,] 44.35644  3.117123 38.34632 50.52228\n[4,] 44.51431  3.079164 38.52404 50.69871\n[5,] 44.60493  3.016152 38.58774 50.55916\n[6,] 44.68798  3.118572 38.40719 50.60227\n```\n\n\n:::\n:::\n\n\nThe posterior means (`Estimate`) are about the same, but the SD's (`Est.Error`) are much larger in the `predict()` output, and the widths of the 95% intervals are too. Let's make a plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThe gray band is the 95% interval for the entire posterior predictive distribution, not just the mean. In a good model, about 95% of the data points should be within those bands.\n\nDiscuss how the jagged lines have to do with the uncertainty in $\\sigma$.\n\nIf we wanted to, we could integrate the `fitted()`-based conditional posterior mean, with the `predict()`-based posterior-predictive distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save the fitted() results\nf <- fitted(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) \n\npredict(fit1.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nIt's the posterior predictive distribution that we use to predict new data points. For example, here's what happens if we use `predict()` without the `newdata` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 47.73398  2.946435 41.96257 53.30345\n[2,] 49.57931  2.907016 43.95716 55.34362\n[3,] 48.45414  2.914526 42.70996 54.26991\n[4,] 47.87854  2.990420 41.90852 53.74147\n[5,] 48.85535  3.022495 43.04634 55.10729\n[6,] 49.75814  3.030254 43.96871 55.72585\n```\n\n\n:::\n:::\n\n\nWe get posterior predictive summaries for each of the original data points. Here's what happens if we set `summary = FALSE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:4000, 1:68] 51.4 47 49.3 48.9 46 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\nThis time, we got 4,000 posterior draws for each. We can reduce that output with the `ndraws` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:6, 1:68] 53.8 45.6 49.9 44.1 42.5 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\nNow wrangle and plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %>% \n  data.frame() %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(row = str_remove(name, \"X\") %>% as.double()) %>% \n  left_join(chinstrap %>% \n              mutate(row = 1:n()),\n            by = join_by(row)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nWith `predict()`, we can use the entire posterior-predictive distribution to simulate new data based on the values of our predictor variable(s). To give you a better sense of what's happening under the hood, here's an `as_draws_df()` based alternative.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 6) %>% \n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nNow take a look at what happens when we plot the densities of several simulated draws.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nas_draws_df(fit1.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 50) %>%  # increase the number of random draws\n  expand_grid(chinstrap %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(chinstrap$bill_length_mm) + c(-2, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nThe similarities and differences among the individual density lines give you a sense of the (un)certainty of the posterior-predictive distribution.\n\n**This may be a good time for you to work on Exercise 1 (see end of the document)**\n\n#Part 4: Beginning to look at priors\n\n## Bayes' rule\n\nBayes' theorem will allow us to determine the plausibility of various values of our parameter(s) of interest, $\\theta$, given the data $d$, which we can express formally as $\\Pr(\\theta \\mid d)$. Bayes' rule takes on the form\n\n$$\n\\Pr(\\theta \\mid d) = \\frac{\\Pr(d \\mid \\theta) \\Pr(\\theta)}{\\Pr(d)}.\n$$\n\nwhere\n\n-   $\\Pr(d \\mid \\theta)$ is the *likelihood*,\n-   $\\Pr(\\theta)$ is the *prior*,\n-   $\\Pr(d)$ is the *average probability of the data*, and\n-   $\\Pr(\\theta \\mid d)$ is the *posterior*.\n\nWe can express this in words as\n\n$$\n\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\n$$\n\nThe denominator $\\Pr(d)$ is a normalizing constant, and dividing by this constant is what converts the posterior $\\Pr(\\theta \\mid d)$ into a probability metric.\n\n## Default priors\n\nTo set your priors with **brms**, the `brm()` function has a `prior` argument. If you don't explicitly use the `prior` argument, `brm()` will use default priors. This is what happened with our `fit1.b` model from above. We used default priors. If you'd like to see what those priors are, execute `fit1.b$prior`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# maybe show str(fit1.b)\nfit1.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\nThus, a fuller expression of our model is\n\n$$\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i \\\\\n\\beta_0 & \\sim \\operatorname{Student-t}(3, 49.5, 3.6) \\\\\n\\beta_1 & \\sim \\operatorname{Uniform}(-\\infty, \\infty) \\\\\n\\sigma & \\sim \\operatorname{Student-t}^+(3, 0, 3.6).\n\\end{align}\n$$\n\nIf we had wanted to see the `brm()` defaults before fitting the model, we could have used the `get_prior()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n:::\n\n\nIf you recall, the normal distribution is a member of the Student-t family, where the $\\nu$ (aka degrees of freedom or normality parameter) is set to $\\infty$. To give you a sense, here are the densities of three members of the Student-t family, with varying $\\nu$ values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(theta = seq(from = -4.5, to = 4.5, length.out = 200),\n         nu = c(3, 10, Inf)) %>% \n  mutate(density = dt(x = theta, df = nu)) %>% \n  \n  ggplot(aes(x = theta, y = density, color = factor(nu))) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(expression(nu), option = \"A\", end = .7) +\n  labs(title = \"3 members of the Student-t family\",\n       x = expression(theta)) +\n  coord_cartesian(xlim = c(-4, 4))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nThus, Student-t distributions have thicker tails when they have smaller $\\nu$ parameters. In the case where $\\nu = 3$, the tails are pretty thick, which means they are more tolerant of more extreme values. And thus priors with small-$\\nu$ parameters will be weaker (i.e., more permissive) than their Gaussian counterparts.\n\nWe can visualize functions from **ggdist** to visualize the default `brm()` priors. We'll start with the `student_t(3, 49.5, 3.6)` $\\beta_0$ prior, and also take the opportunity to compare that with a slightly stronger `normal(49.5, 3.6)` alternative.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 49.5, 3.6)),\n  prior(normal(49.5, 3.6))) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nSee how that $n = 3$ parameter in the default prior let do much thicker tails than it's Gaussian counterpart. We can make the same kind of plot for our default $\\sigma$ prior and its half-Gaussian counterpart.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(prior(student_t(3, 0, 3.6), lb = 0),  # note our use of the lb = 0 argument\n  prior(normal(0, 3.6), lb = 0)) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nHere's how we could have explicitly set our priors by hand.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2.b <- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g,\n  prior = prior(student_t(3, 49.5, 3.6), class = Intercept) +\n    prior(student_t(3, 0, 3.6), class = sigma, lb = 0)\n)\n```\n:::\n\n\nCompare the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.15      3.47    25.25    39.08 1.00     4666     2718\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4796     2776\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.25     2.49     3.48 1.00     1813     1761\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit2.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.23      3.58    25.44    39.20 1.00     5050     2927\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5054     2899\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.92      0.25     2.47     3.45 1.00     2345     2100\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## QUESTION 2 Are the priors the same? What do you think is going on?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n\n```{.r .cell-code}\nfit2.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n         user\n         user\n```\n\n\n:::\n:::\n\n\n### Answer:\n\nThe priors seem to be the same between these two models. For model 1, all three priors are by default. For model 2, we are specifying the priors for the intercept and for sigma, while we are still using the default prior for the slope.\n\nIf you want to learn more about the default prior settings for **brms**, read through the `set_prior` section of the **brms** reference manual (https://CRAN.R-project.org/package=brms/brms.pdf).\n\n# EXERCISE 1\n\nIn the previous lab, we made a subset of the `penguins` data called `gentoo`, which was only the cases for which `species == \"Gentoo\"`. Do that again and refit the Bayesian model to those data. Remake some of the figures (From Part 3) in this file with the new version of the model?\n\n### Answer/ Your solution below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# subset the data\ngentoo = penguins %>% \n  filter(species == \"Gentoo\") %>%\n  drop_na(bill_length_mm, body_mass_g)\n\n# Bayes\nfit3.b <- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTrying to compile a simple C file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior Means\nb0    = posterior_summary(fit3.b)[1, 1]\nb1    = posterior_summary(fit3.b)[2, 1]\nsigma = posterior_summary(fit3.b)[3, 1]\n\ncrossing(body_mass_g    = seq(from = 3800, to = 6400, length.out = 200),\n         bill_length_mm = seq(from = 40, to = 60, length.out = 200))  %>% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %>% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = gentoo,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(gentoo$body_mass_g),\n                  ylim = range(gentoo$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameter Distributions\nas_draws_df(fit3.b) %>% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %>% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %>% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model Expectations\nnd <- tibble(body_mass_g = seq(\n  from = min(gentoo$body_mass_g),\n  to = max(gentoo$body_mass_g),\n  length.out = 100))\n\nfitted(fit3.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfitted(fit3.b, \n       newdata = nd,\n       summary = F) %>% \n  data.frame() %>% \n  set_names(pull(nd, body_mass_g)) %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(body_mass_g = as.double(name)) %>%\n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(gentoo$bill_length_mm)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-40-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior-predictive Distributions\nf <- fitted(fit3.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) \n\npredict(fit3.b, newdata = nd) %>% \n  data.frame() %>% \n  bind_cols(nd) %>% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(gentoo$bill_length_mm))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit3.b, summary = FALSE, ndraws = 6) %>% \n  data.frame() %>% \n  mutate(draw = 1:n()) %>% \n  pivot_longer(-draw) %>% \n  mutate(row = str_remove(name, \"X\") %>% as.double()) %>% \n  left_join(gentoo %>% \n              mutate(row = 1:n()),\n            by = join_by(row)) %>% \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nas_draws_df(fit3.b) %>% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %>% \n  select(.draw, beta0, beta1, sigma) %>% \n  slice_sample(n = 50) %>%  # increase the number of random draws\n  expand_grid(gentoo %>% select(body_mass_g)) %>% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %>% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(gentoo$bill_length_mm) + c(-2, 2))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prior parameter distribution\nfit3.b$prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 47.3, 3.1) Intercept                                        \n    student_t(3, 0, 3.1)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n\n\n:::\n\n```{.r .cell-code}\nc(prior(student_t(3, 47.3, 3.1))) %>% \n  parse_dist() %>% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n\n```{.r .cell-code}\nc(prior(student_t(3, 0, 3.1), lb = 0)) %>% \n  parse_dist() %>% \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30))\n```\n\n::: {.cell-output-display}\n![](Bayes_Lab_2_files/figure-html/unnamed-chunk-44-2.png){width=672}\n:::\n:::\n\n\n## References\n\nKruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*. Academic Press. <https://sites.google.com/site/doingbayesiandataanalysis/>\n\n## Session information\n",
    "supporting": [
      "Bayes_Lab_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}